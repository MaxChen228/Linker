# TASK-30B: è©³ç´°ä¾è³´åˆ†æå’Œå½±éŸ¿è©•ä¼°

- **å„ªå…ˆç´š**: ğŸ”´ CRITICAL
- **é è¨ˆæ™‚é–“**: 6-8 å°æ™‚
- **é—œè¯çµ„ä»¶**: æ‰€æœ‰æ¶‰åŠ JSON çš„æ¨¡çµ„
- **çˆ¶ä»»å‹™**: TASK-30 ç´” Database ç³»çµ±é‡æ§‹å°ˆæ¡ˆ
- **å‰ç½®æ¢ä»¶**: TASK-30A å®Œæˆ
- **å¾ŒçºŒä»»å‹™**: TASK-30C, TASK-30D

---

## ğŸ¯ ä»»å‹™ç›®æ¨™

é€²è¡Œå…¨é¢çš„ä»£ç¢¼ä¾è³´åˆ†æï¼Œç²¾ç¢ºè­˜åˆ¥æ‰€æœ‰ JSON ç›¸é—œçš„ä»£ç¢¼è·¯å¾‘ã€æ•¸æ“šæµå’Œäº¤äº’é—œä¿‚ï¼Œç‚ºå¾ŒçºŒçš„ç§»é™¤å·¥ä½œæä¾›è©³ç´°çš„æŠ€è¡“è·¯ç·šåœ–ã€‚

## âœ… é©—æ”¶æ¨™æº–

### ä¾è³´æ˜ å°„
- [ ] ç”Ÿæˆå®Œæ•´çš„ JSON ä¾è³´é—œä¿‚åœ–
- [ ] è­˜åˆ¥æ‰€æœ‰ç›´æ¥å’Œé–“æ¥çš„ JSON å¼•ç”¨
- [ ] åˆ†æè·¨æ¨¡çµ„çš„æ•¸æ“šæµå‘
- [ ] æ¨™è¨˜é«˜é¢¨éšªå’Œä½é¢¨éšªçš„è®Šæ›´é»

### å½±éŸ¿è©•ä¼°
- [ ] è©•ä¼°æ¯å€‹è®Šæ›´çš„æ¥­å‹™å½±éŸ¿
- [ ] é‡åŒ–æŠ€è¡“é¢¨éšªç­‰ç´š
- [ ] ä¼°ç®—é‡æ§‹å·¥ä½œé‡
- [ ] è­˜åˆ¥é—œéµè·¯å¾‘å’Œä¾è³´é †åº

### æŠ€è¡“æ–‡æª”
- [ ] å‰µå»ºè©³ç´°çš„é‡æ§‹è·¯ç·šåœ–
- [ ] ç”Ÿæˆä»£ç¢¼è®Šæ›´æ¸…å–®
- [ ] åˆ¶å®šæ¸¬è©¦ç­–ç•¥
- [ ] å»ºç«‹é¢¨éšªç·©è§£è¨ˆåŠƒ

## ğŸ“‹ è©³ç´°åŸ·è¡Œæ­¥é©Ÿ

### 1ï¸âƒ£ ä»£ç¢¼æƒæå’Œä¾è³´ç™¼ç¾ (2-3 å°æ™‚)

#### å…¨å±€ JSON ä¾è³´æƒæ
```bash
# å‰µå»ºåˆ†æçµæœç›®éŒ„
mkdir -p analysis/dependency_analysis/$(date +%Y%m%d_%H%M%S)
ANALYSIS_DIR="analysis/dependency_analysis/$(date +%Y%m%d_%H%M%S)"

# æƒææ‰€æœ‰ JSON ç›¸é—œçš„ç›´æ¥å¼•ç”¨
echo "=== Direct JSON Imports ===" > "$ANALYSIS_DIR/json_dependencies.txt"
rg "import json" --type py -n >> "$ANALYSIS_DIR/json_dependencies.txt"
rg "from json" --type py -n >> "$ANALYSIS_DIR/json_dependencies.txt"

# æƒæ JSON æ–¹æ³•èª¿ç”¨
echo -e "\n=== JSON Method Calls ===" >> "$ANALYSIS_DIR/json_dependencies.txt"
rg "json\.(load|loads|dump|dumps)" --type py -n -A 2 -B 2 >> "$ANALYSIS_DIR/json_dependencies.txt"

# æƒæ JSON æ–‡ä»¶è·¯å¾‘å¼•ç”¨
echo -e "\n=== JSON File References ===" >> "$ANALYSIS_DIR/json_dependencies.txt"
rg "\.json" --type py -n -A 1 -B 1 >> "$ANALYSIS_DIR/json_dependencies.txt"

# æƒæ knowledge.py ç›¸é—œå¼•ç”¨
echo -e "\n=== Knowledge.py References ===" >> "$ANALYSIS_DIR/json_dependencies.txt"
rg "from core\.knowledge" --type py -n >> "$ANALYSIS_DIR/json_dependencies.txt"
rg "import.*knowledge" --type py -n >> "$ANALYSIS_DIR/json_dependencies.txt"
```

#### æ·±åº¦ä¾è³´åˆ†æ
```python
# å‰µå»ºä¾è³´åˆ†æè…³æœ¬
cat > analysis/analyze_dependencies.py << 'EOF'
#!/usr/bin/env python3
"""
æ·±åº¦ JSON ä¾è³´åˆ†æå·¥å…·
åˆ†æä»£ç¢¼ä¸­æ‰€æœ‰èˆ‡ JSON ç›¸é—œçš„ä¾è³´é—œä¿‚
"""

import ast
import os
import json
from pathlib import Path
from collections import defaultdict, deque
from typing import Dict, List, Set, Tuple

class DependencyAnalyzer:
    def __init__(self, root_dir: str):
        self.root_dir = Path(root_dir)
        self.dependencies = defaultdict(set)
        self.reverse_dependencies = defaultdict(set)
        self.json_related_files = set()
        self.risk_levels = {}
        
    def analyze_file(self, file_path: Path) -> Dict:
        """åˆ†æå–®å€‹ Python æ–‡ä»¶çš„ä¾è³´"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            tree = ast.parse(content)
            analysis = {
                'file': str(file_path.relative_to(self.root_dir)),
                'imports': [],
                'json_usage': [],
                'functions': [],
                'classes': [],
                'risk_indicators': []
            }
            
            for node in ast.walk(tree):
                # åˆ†æ import èªå¥
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        analysis['imports'].append(alias.name)
                        if 'json' in alias.name:
                            analysis['json_usage'].append(f"import {alias.name}")
                            
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    for alias in node.names:
                        full_name = f"{module}.{alias.name}"
                        analysis['imports'].append(full_name)
                        if 'json' in module or 'json' in alias.name:
                            analysis['json_usage'].append(f"from {module} import {alias.name}")
                
                # åˆ†æå‡½æ•¸å®šç¾©
                elif isinstance(node, ast.FunctionDef):
                    analysis['functions'].append(node.name)
                    
                # åˆ†æé¡å®šç¾©
                elif isinstance(node, ast.ClassDef):
                    analysis['classes'].append(node.name)
                    
                # åˆ†æ JSON æ–¹æ³•èª¿ç”¨
                elif isinstance(node, ast.Call):
                    if isinstance(node.func, ast.Attribute):
                        if isinstance(node.func.value, ast.Name):
                            if node.func.value.id == 'json':
                                method = node.func.attr
                                analysis['json_usage'].append(f"json.{method}()")
                                analysis['risk_indicators'].append(f"Direct JSON call: json.{method}")
                                
            # æª¢æŸ¥æ–‡ä»¶å…§å®¹ä¸­çš„ JSON ç›¸é—œå­—ç¬¦ä¸²
            if '.json' in content or 'knowledge.json' in content or 'practice_log.json' in content:
                analysis['risk_indicators'].append("JSON file path reference")
                
            if 'legacy_manager' in content:
                analysis['risk_indicators'].append("Legacy manager reference")
                
            if 'fallback' in content.lower() and 'json' in content.lower():
                analysis['risk_indicators'].append("JSON fallback logic")
                
            return analysis
            
        except Exception as e:
            return {
                'file': str(file_path.relative_to(self.root_dir)),
                'error': str(e),
                'risk_indicators': ['Parse error - manual review needed']
            }
    
    def calculate_risk_level(self, analysis: Dict) -> str:
        """è¨ˆç®—é¢¨éšªç­‰ç´š"""
        risk_score = 0
        
        # JSON ä½¿ç”¨é »ç‡
        risk_score += len(analysis.get('json_usage', [])) * 2
        
        # é¢¨éšªæŒ‡æ¨™
        risk_indicators = analysis.get('risk_indicators', [])
        risk_score += len(risk_indicators) * 3
        
        # ç‰¹æ®Šé¢¨éšª
        for indicator in risk_indicators:
            if 'fallback' in indicator.lower():
                risk_score += 5
            if 'legacy' in indicator.lower():
                risk_score += 4
            if 'Direct JSON call' in indicator:
                risk_score += 3
                
        # æ–‡ä»¶é‡è¦æ€§ï¼ˆåŸºæ–¼è·¯å¾‘ï¼‰
        file_path = analysis['file']
        if 'core/' in file_path:
            risk_score += 2
        if 'web/' in file_path:
            risk_score += 1
        if 'database' in file_path:
            risk_score += 3
            
        if risk_score >= 10:
            return "ğŸ”´ HIGH"
        elif risk_score >= 5:
            return "ğŸŸ  MEDIUM"
        elif risk_score > 0:
            return "ğŸŸ¡ LOW"
        else:
            return "ğŸŸ¢ NONE"
    
    def analyze_all_files(self) -> Dict:
        """åˆ†ææ‰€æœ‰ Python æ–‡ä»¶"""
        results = {
            'files': [],
            'summary': {
                'total_files': 0,
                'json_related_files': 0,
                'high_risk_files': 0,
                'medium_risk_files': 0,
                'low_risk_files': 0
            },
            'dependencies': {},
            'critical_paths': []
        }
        
        # æƒææ‰€æœ‰ Python æ–‡ä»¶
        for py_file in self.root_dir.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['.venv', '__pycache__', '.git']):
                continue
                
            analysis = self.analyze_file(py_file)
            risk_level = self.calculate_risk_level(analysis)
            analysis['risk_level'] = risk_level
            
            results['files'].append(analysis)
            results['summary']['total_files'] += 1
            
            if analysis.get('json_usage') or analysis.get('risk_indicators'):
                results['summary']['json_related_files'] += 1
                
                if 'HIGH' in risk_level:
                    results['summary']['high_risk_files'] += 1
                elif 'MEDIUM' in risk_level:
                    results['summary']['medium_risk_files'] += 1
                elif 'LOW' in risk_level:
                    results['summary']['low_risk_files'] += 1
        
        return results
    
    def generate_dependency_graph(self, results: Dict) -> str:
        """ç”Ÿæˆä¾è³´é—œä¿‚åœ–"""
        graph = "# JSON ä¾è³´é—œä¿‚åœ–\n\n"
        graph += "```mermaid\ngraph TD\n"
        
        # æŒ‰é¢¨éšªç­‰ç´šåˆ†çµ„
        high_risk = [f for f in results['files'] if 'HIGH' in f.get('risk_level', '')]
        medium_risk = [f for f in results['files'] if 'MEDIUM' in f.get('risk_level', '')]
        
        # æ·»åŠ é«˜é¢¨éšªç¯€é»
        for i, file_info in enumerate(high_risk):
            file_name = file_info['file'].replace('/', '_').replace('.py', '')
            graph += f"    {file_name}[{file_info['file']}]\n"
            graph += f"    {file_name} --> JSON_SYSTEM\n"
            
        # æ·»åŠ ä¸­é¢¨éšªç¯€é»
        for i, file_info in enumerate(medium_risk):
            file_name = file_info['file'].replace('/', '_').replace('.py', '')
            graph += f"    {file_name}[{file_info['file']}]\n"
            graph += f"    {file_name} --> JSON_SYSTEM\n"
            
        graph += "    JSON_SYSTEM[JSON System]\n"
        graph += "    JSON_SYSTEM --> DATABASE[Database System]\n"
        graph += "```\n"
        
        return graph

def main():
    analyzer = DependencyAnalyzer('.')
    results = analyzer.analyze_all_files()
    
    # ä¿å­˜è©³ç´°åˆ†æçµæœ
    with open('analysis/detailed_dependency_analysis.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # ç”Ÿæˆæ‘˜è¦å ±å‘Š
    summary = f"""# JSON ä¾è³´åˆ†ææ‘˜è¦

## ğŸ“Š çµ±è¨ˆæ‘˜è¦
- ç¸½æ–‡ä»¶æ•¸: {results['summary']['total_files']}
- JSON ç›¸é—œæ–‡ä»¶: {results['summary']['json_related_files']}
- é«˜é¢¨éšªæ–‡ä»¶: {results['summary']['high_risk_files']}
- ä¸­é¢¨éšªæ–‡ä»¶: {results['summary']['medium_risk_files']}
- ä½é¢¨éšªæ–‡ä»¶: {results['summary']['low_risk_files']}

## ğŸ”´ é«˜é¢¨éšªæ–‡ä»¶
"""
    
    high_risk_files = [f for f in results['files'] if 'HIGH' in f.get('risk_level', '')]
    for file_info in high_risk_files:
        summary += f"\n### {file_info['file']}\n"
        summary += f"- **é¢¨éšªç­‰ç´š**: {file_info['risk_level']}\n"
        summary += f"- **JSON ä½¿ç”¨**: {len(file_info.get('json_usage', []))}\n"
        summary += f"- **é¢¨éšªæŒ‡æ¨™**: {len(file_info.get('risk_indicators', []))}\n"
        
        if file_info.get('json_usage'):
            summary += "- **JSON èª¿ç”¨**:\n"
            for usage in file_info['json_usage']:
                summary += f"  - {usage}\n"
                
        if file_info.get('risk_indicators'):
            summary += "- **é¢¨éšªå› ç´ **:\n"
            for risk in file_info['risk_indicators']:
                summary += f"  - {risk}\n"
    
    # ç”Ÿæˆä¾è³´åœ–
    dependency_graph = analyzer.generate_dependency_graph(results)
    summary += "\n" + dependency_graph
    
    # ä¿å­˜æ‘˜è¦
    with open('analysis/dependency_analysis_summary.md', 'w') as f:
        f.write(summary)
    
    print("âœ… ä¾è³´åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ è©³ç´°çµæœ: analysis/detailed_dependency_analysis.json")
    print(f"ğŸ“„ æ‘˜è¦å ±å‘Š: analysis/dependency_analysis_summary.md")
    print(f"\nğŸ“Š å¿«é€Ÿçµ±è¨ˆ:")
    print(f"   - é«˜é¢¨éšªæ–‡ä»¶: {results['summary']['high_risk_files']}")
    print(f"   - ä¸­é¢¨éšªæ–‡ä»¶: {results['summary']['medium_risk_files']}")
    print(f"   - JSON ç›¸é—œæ–‡ä»¶: {results['summary']['json_related_files']}")

if __name__ == "__main__":
    main()
EOF

chmod +x analysis/analyze_dependencies.py
```

#### åŸ·è¡Œæ·±åº¦åˆ†æ
```bash
# é‹è¡Œä¾è³´åˆ†æ
cd /Users/chenliangyu/Desktop/linker
python3 analysis/analyze_dependencies.py

# ç”Ÿæˆèª¿ç”¨éˆåˆ†æ
echo "=== Function Call Chain Analysis ===" > "$ANALYSIS_DIR/call_chain_analysis.txt"

# æŸ¥æ‰¾æ‰€æœ‰èª¿ç”¨ json.load çš„å‡½æ•¸
rg "json\.load" --type py -A 5 -B 5 >> "$ANALYSIS_DIR/call_chain_analysis.txt"

# æŸ¥æ‰¾æ‰€æœ‰èª¿ç”¨é€™äº›å‡½æ•¸çš„åœ°æ–¹
rg "save_mistake|load_knowledge|load_practice" --type py -A 3 -B 3 >> "$ANALYSIS_DIR/call_chain_analysis.txt"
```

### 2ï¸âƒ£ æ¥­å‹™æµç¨‹å½±éŸ¿åˆ†æ (2 å°æ™‚)

#### æ ¸å¿ƒæ¥­å‹™æµç¨‹æ˜ å°„
```bash
cat > analysis/business_impact_analysis.py << 'EOF'
#!/usr/bin/env python3
"""
æ¥­å‹™æµç¨‹å½±éŸ¿åˆ†æ
åˆ†æ JSON ç§»é™¤å°æ ¸å¿ƒæ¥­å‹™åŠŸèƒ½çš„å½±éŸ¿
"""

import json
from pathlib import Path

class BusinessImpactAnalyzer:
    def __init__(self):
        self.critical_flows = [
            "ç·´ç¿’é¡Œç”Ÿæˆå’Œè©•åˆ†",
            "éŒ¯èª¤åˆ†æå’Œä¿å­˜",
            "çŸ¥è­˜é»ç®¡ç†",
            "è¤‡ç¿’è¨ˆåŠƒç”Ÿæˆ",
            "çµ±è¨ˆæ•¸æ“šæŸ¥è©¢",
            "ç”¨æˆ¶é€²åº¦è¿½è¹¤"
        ]
        
        self.api_endpoints = []
        self.data_models = []
        
    def analyze_api_endpoints(self):
        """åˆ†æ API ç«¯é»çš„ JSON ä¾è³´"""
        router_files = [
            'web/routers/api_knowledge.py',
            'web/routers/knowledge.py',
            'web/routers/practice.py',
            'web/routers/pages.py',
            'web/routers/calendar.py'
        ]
        
        endpoint_analysis = {}
        
        for router_file in router_files:
            try:
                with open(router_file, 'r') as f:
                    content = f.read()
                
                # æª¢æŸ¥ JSON ç›¸é—œçš„ç«¯é»
                if 'knowledge' in content and ('json' in content.lower() or 'legacy' in content.lower()):
                    endpoint_analysis[router_file] = {
                        'has_json_dependency': True,
                        'risk_level': 'HIGH' if 'legacy' in content else 'MEDIUM',
                        'affected_endpoints': []
                    }
                    
                    # æå–è·¯ç”±å®šç¾©
                    lines = content.split('\n')
                    for i, line in enumerate(lines):
                        if '@router.' in line and ('get' in line.lower() or 'post' in line.lower()):
                            endpoint_analysis[router_file]['affected_endpoints'].append(line.strip())
                            
            except FileNotFoundError:
                continue
                
        return endpoint_analysis
    
    def analyze_data_flows(self):
        """åˆ†ææ•¸æ“šæµå‘"""
        data_flows = {
            "ç”¨æˆ¶æäº¤ç­”æ¡ˆ": {
                "entry_point": "POST /api/knowledge/practice",
                "data_path": [
                    "web/routers/practice.py",
                    "core/ai_service.py (grading)",
                    "core/knowledge.py (save_mistake)",
                    "data/knowledge.json (JSON storage)"
                ],
                "database_alternative": [
                    "web/routers/practice.py",
                    "core/ai_service.py (grading)",
                    "core/database/adapter.py (save_mistake_async)",
                    "PostgreSQL database"
                ],
                "migration_complexity": "MEDIUM"
            },
            "ç²å–è¤‡ç¿’å€™é¸": {
                "entry_point": "GET /api/knowledge/review-candidates",
                "data_path": [
                    "web/routers/api_knowledge.py",
                    "core/knowledge.py (get_review_candidates)",
                    "data/knowledge.json (JSON storage)"
                ],
                "database_alternative": [
                    "web/routers/api_knowledge.py",
                    "core/database/adapter.py (get_review_candidates_async)",
                    "PostgreSQL database"
                ],
                "migration_complexity": "LOW"
            },
            "çµ±è¨ˆæ•¸æ“šæŸ¥è©¢": {
                "entry_point": "GET /api/knowledge/statistics",
                "data_path": [
                    "web/routers/api_knowledge.py",
                    "core/knowledge.py (get_statistics)",
                    "data/knowledge.json + data/practice_log.json"
                ],
                "database_alternative": [
                    "web/routers/api_knowledge.py",
                    "core/database/adapter.py (get_statistics_async)",
                    "PostgreSQL aggregated queries"
                ],
                "migration_complexity": "HIGH"
            }
        }
        
        return data_flows
    
    def generate_impact_report(self):
        """ç”Ÿæˆå½±éŸ¿è©•ä¼°å ±å‘Š"""
        api_analysis = self.analyze_api_endpoints()
        data_flows = self.analyze_data_flows()
        
        report = f"""# æ¥­å‹™æµç¨‹å½±éŸ¿åˆ†æå ±å‘Š

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### é—œéµç™¼ç¾
- **é«˜é¢¨éšªç«¯é»**: {sum(1 for a in api_analysis.values() if a.get('risk_level') == 'HIGH')}
- **ä¸­é¢¨éšªç«¯é»**: {sum(1 for a in api_analysis.values() if a.get('risk_level') == 'MEDIUM')}
- **æ ¸å¿ƒæ•¸æ“šæµ**: {len(data_flows)}

### ç¸½é«”é¢¨éšªè©•ä¼°
- **æ¥­å‹™é€£çºŒæ€§é¢¨éšª**: ğŸŸ¡ MEDIUM (æœ‰å®Œæ•´çš„è³‡æ–™åº«æ›¿ä»£æ–¹æ¡ˆ)
- **æ•¸æ“šå®Œæ•´æ€§é¢¨éšª**: ğŸŸ¢ LOW (å®Œæ•´çš„å‚™ä»½å’Œé©—è­‰æ©Ÿåˆ¶)
- **æ€§èƒ½å½±éŸ¿**: ğŸŸ¢ POSITIVE (è³‡æ–™åº«æŸ¥è©¢æ¯” JSON æ–‡ä»¶è®€å–æ›´é«˜æ•ˆ)

## ğŸ” è©³ç´°åˆ†æ

### API ç«¯é»å½±éŸ¿
"""
        
        for router_file, analysis in api_analysis.items():
            report += f"\n#### {router_file}\n"
            report += f"- **é¢¨éšªç­‰ç´š**: {analysis.get('risk_level', 'UNKNOWN')}\n"
            report += f"- **JSON ä¾è³´**: {'æ˜¯' if analysis.get('has_json_dependency') else 'å¦'}\n"
            report += f"- **å—å½±éŸ¿ç«¯é»**: {len(analysis.get('affected_endpoints', []))}\n"
            
            if analysis.get('affected_endpoints'):
                report += "- **ç«¯é»åˆ—è¡¨**:\n"
                for endpoint in analysis['affected_endpoints']:
                    report += f"  - {endpoint}\n"
        
        report += "\n### æ ¸å¿ƒæ•¸æ“šæµåˆ†æ\n"
        
        for flow_name, flow_info in data_flows.items():
            report += f"\n#### {flow_name}\n"
            report += f"- **å…¥å£é»**: {flow_info['entry_point']}\n"
            report += f"- **é·ç§»è¤‡é›œåº¦**: {flow_info['migration_complexity']}\n"
            report += "- **ç•¶å‰è·¯å¾‘**:\n"
            for step in flow_info['data_path']:
                report += f"  1. {step}\n"
            report += "- **ç›®æ¨™è·¯å¾‘**:\n"
            for step in flow_info['database_alternative']:
                report += f"  1. {step}\n"
        
        report += """

## ğŸ¯ é·ç§»ç­–ç•¥å»ºè­°

### éšæ®µæ€§é·ç§»
1. **éšæ®µ 1**: å¾Œç«¯ API åˆ‡æ›ï¼ˆä¿æŒå‰ç«¯ä¸è®Šï¼‰
2. **éšæ®µ 2**: æ•¸æ“šæºåˆ‡æ›ï¼ˆJSON â†’ Databaseï¼‰
3. **éšæ®µ 3**: æ¸…ç†èˆŠä»£ç¢¼å’Œæ–‡ä»¶

### é¢¨éšªç·©è§£
1. **A/B æ¸¬è©¦**: ä½¿ç”¨åŠŸèƒ½é–‹é—œé€æ­¥åˆ‡æ›
2. **é›™å¯«ç­–ç•¥**: è‡¨æ™‚åŒæ™‚å¯«å…¥ JSON å’Œè³‡æ–™åº«
3. **ç›£æ§å‘Šè­¦**: å¯¦æ™‚ç›£æ§éŒ¯èª¤ç‡å’Œæ€§èƒ½

### å›æ»¾è¨ˆåŠƒ
- ä¿æŒ JSON æ–‡ä»¶å‚™ä»½
- ä¿ç•™åˆ‡æ›é–‹é—œ
- è‡ªå‹•åŒ–å›æ»¾è…³æœ¬
"""
        
        return report

def main():
    analyzer = BusinessImpactAnalyzer()
    report = analyzer.generate_impact_report()
    
    with open('analysis/business_impact_analysis.md', 'w') as f:
        f.write(report)
    
    print("âœ… æ¥­å‹™å½±éŸ¿åˆ†æå®Œæˆï¼")
    print("ğŸ“„ å ±å‘Šä½ç½®: analysis/business_impact_analysis.md")

if __name__ == "__main__":
    main()
EOF

chmod +x analysis/business_impact_analysis.py
python3 analysis/business_impact_analysis.py
```

### 3ï¸âƒ£ æ¸¬è©¦å½±éŸ¿åˆ†æ (1-2 å°æ™‚)

#### æ¸¬è©¦è¦†è“‹åº¦åˆ†æ
```bash
# åˆ†æç¾æœ‰æ¸¬è©¦ä¸­çš„ JSON ä¾è³´
echo "=== Test JSON Dependencies ===" > "$ANALYSIS_DIR/test_analysis.txt"

# æƒææ¸¬è©¦æ–‡ä»¶ä¸­çš„ JSON ç›¸é—œä»£ç¢¼
find tests/ -name "*.py" -exec grep -l "json\|knowledge\.py\|legacy" {} \; >> "$ANALYSIS_DIR/test_files_with_json.txt"

# åˆ†ææ¯å€‹æ¸¬è©¦æ–‡ä»¶çš„ä¾è³´
for test_file in $(cat "$ANALYSIS_DIR/test_files_with_json.txt"); do
    echo "=== $test_file ===" >> "$ANALYSIS_DIR/test_analysis.txt"
    grep -n "json\|knowledge\|legacy\|fallback" "$test_file" >> "$ANALYSIS_DIR/test_analysis.txt"
    echo "" >> "$ANALYSIS_DIR/test_analysis.txt"
done

# é‹è¡Œæ¸¬è©¦è¦†è“‹ç‡åˆ†æ
pytest --cov=core --cov=web --cov-report=html --cov-report=term > "$ANALYSIS_DIR/current_test_coverage.txt"
```

#### å‰µå»ºæ¸¬è©¦é‡æ§‹è¨ˆåŠƒ
```python
cat > analysis/test_migration_plan.py << 'EOF'
#!/usr/bin/env python3
"""
æ¸¬è©¦é‡æ§‹è¨ˆåŠƒç”Ÿæˆå™¨
åˆ†æç¾æœ‰æ¸¬è©¦ä¸¦ç”Ÿæˆé‡æ§‹ç­–ç•¥
"""

import os
import re
from pathlib import Path

class TestMigrationPlanner:
    def __init__(self):
        self.test_files = []
        self.migration_plan = {}
        
    def analyze_test_file(self, file_path: Path):
        """åˆ†æå–®å€‹æ¸¬è©¦æ–‡ä»¶"""
        with open(file_path, 'r') as f:
            content = f.read()
        
        analysis = {
            'file': str(file_path),
            'json_dependencies': [],
            'knowledge_imports': [],
            'database_usage': [],
            'migration_complexity': 'LOW',
            'required_changes': []
        }
        
        # æª¢æŸ¥ JSON ä¾è³´
        if 'import json' in content or 'from json' in content:
            analysis['json_dependencies'].append('Direct JSON import')
            analysis['migration_complexity'] = 'MEDIUM'
        
        # æª¢æŸ¥ knowledge.py å¼•ç”¨
        if 'from core.knowledge' in content or 'core.knowledge' in content:
            analysis['knowledge_imports'].append('Legacy knowledge.py import')
            analysis['migration_complexity'] = 'HIGH'
            analysis['required_changes'].append('Replace with DatabaseAdapter')
        
        # æª¢æŸ¥æ˜¯å¦å·²ä½¿ç”¨è³‡æ–™åº«
        if 'DatabaseAdapter' in content or 'asyncpg' in content:
            analysis['database_usage'].append('Already using database')
            analysis['migration_complexity'] = 'LOW'
        
        # æª¢æŸ¥æ¸¬è©¦ fixtures
        if 'temp_data_dir' in content or 'mock_env_vars' in content:
            analysis['required_changes'].append('Update test fixtures')
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ–°çš„ fixtures
        if 'USE_DATABASE' in content:
            analysis['required_changes'].append('Add database test fixtures')
        
        return analysis
    
    def scan_all_tests(self):
        """æƒææ‰€æœ‰æ¸¬è©¦æ–‡ä»¶"""
        test_dir = Path('tests')
        test_files = list(test_dir.rglob('*.py'))
        
        for test_file in test_files:
            if '__pycache__' in str(test_file):
                continue
            analysis = self.analyze_test_file(test_file)
            self.test_files.append(analysis)
    
    def generate_migration_plan(self):
        """ç”Ÿæˆæ¸¬è©¦é·ç§»è¨ˆåŠƒ"""
        plan = {
            'high_complexity': [],
            'medium_complexity': [],
            'low_complexity': [],
            'summary': {
                'total_files': len(self.test_files),
                'need_migration': 0,
                'database_ready': 0
            }
        }
        
        for test_analysis in self.test_files:
            complexity = test_analysis['migration_complexity']
            
            if complexity == 'HIGH':
                plan['high_complexity'].append(test_analysis)
            elif complexity == 'MEDIUM':
                plan['medium_complexity'].append(test_analysis)
            else:
                plan['low_complexity'].append(test_analysis)
            
            if test_analysis['json_dependencies'] or test_analysis['knowledge_imports']:
                plan['summary']['need_migration'] += 1
            
            if test_analysis['database_usage']:
                plan['summary']['database_ready'] += 1
        
        return plan
    
    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦é·ç§»å ±å‘Š"""
        self.scan_all_tests()
        plan = self.generate_migration_plan()
        
        report = f"""# æ¸¬è©¦ç³»çµ±é·ç§»è¨ˆåŠƒ

## ğŸ“Š ç¸½è¦½
- **ç¸½æ¸¬è©¦æ–‡ä»¶**: {plan['summary']['total_files']}
- **éœ€è¦é·ç§»**: {plan['summary']['need_migration']}
- **å·²æº–å‚™å°±ç·’**: {plan['summary']['database_ready']}
- **é«˜è¤‡é›œåº¦**: {len(plan['high_complexity'])}
- **ä¸­è¤‡é›œåº¦**: {len(plan['medium_complexity'])}
- **ä½è¤‡é›œåº¦**: {len(plan['low_complexity'])}

## ğŸ”´ é«˜å„ªå…ˆç´šé·ç§» (HIGH Complexity)
"""
        
        for test in plan['high_complexity']:
            report += f"\n### {test['file']}\n"
            report += f"- **è¤‡é›œåº¦**: {test['migration_complexity']}\n"
            if test['json_dependencies']:
                report += f"- **JSON ä¾è³´**: {', '.join(test['json_dependencies'])}\n"
            if test['knowledge_imports']:
                report += f"- **Knowledge å¼•ç”¨**: {', '.join(test['knowledge_imports'])}\n"
            if test['required_changes']:
                report += "- **éœ€è¦çš„è®Šæ›´**:\n"
                for change in test['required_changes']:
                    report += f"  - {change}\n"
        
        report += f"""

## ğŸŸ  ä¸­å„ªå…ˆç´šé·ç§» (MEDIUM Complexity)
"""
        
        for test in plan['medium_complexity']:
            report += f"\n### {test['file']}\n"
            report += f"- **è¤‡é›œåº¦**: {test['migration_complexity']}\n"
            if test['required_changes']:
                report += "- **éœ€è¦çš„è®Šæ›´**:\n"
                for change in test['required_changes']:
                    report += f"  - {change}\n"
        
        report += """

## ğŸ¯ æ¸¬è©¦é·ç§»ç­–ç•¥

### 1. æ›´æ–° conftest.py
- æ·»åŠ è³‡æ–™åº«æ¸¬è©¦ fixtures
- ç§»é™¤ JSON ç›¸é—œçš„ fixtures
- çµ±ä¸€æ¸¬è©¦ç’°å¢ƒé…ç½®

### 2. åˆ†éšæ®µé·ç§»
1. **éšæ®µ 1**: æ›´æ–°é«˜è¤‡é›œåº¦æ¸¬è©¦
2. **éšæ®µ 2**: æ‰¹é‡æ›´æ–°ä¸­è¤‡é›œåº¦æ¸¬è©¦
3. **éšæ®µ 3**: æ¸…ç†å’Œå„ªåŒ–

### 3. æ–°çš„æ¸¬è©¦ç­–ç•¥
- ä½¿ç”¨ pytest-asyncio æ”¯æ´ç•°æ­¥æ¸¬è©¦
- ä½¿ç”¨ pytest-mock æ›¿ä»£ JSON æ–‡ä»¶æ¨¡æ“¬
- å¢åŠ è³‡æ–™åº«äº‹å‹™éš”é›¢

### 4. æŒçºŒé©—è­‰
- æ¯æ¬¡é·ç§»å¾Œé‹è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
- ç›£æ§æ¸¬è©¦è¦†è“‹ç‡è®ŠåŒ–
- ç¢ºä¿æ¸¬è©¦æ€§èƒ½ä¸é™ä½
"""
        
        return report

def main():
    planner = TestMigrationPlanner()
    report = planner.generate_report()
    
    with open('analysis/test_migration_plan.md', 'w') as f:
        f.write(report)
    
    print("âœ… æ¸¬è©¦é·ç§»è¨ˆåŠƒç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“„ å ±å‘Šä½ç½®: analysis/test_migration_plan.md")

if __name__ == "__main__":
    main()
EOF

chmod +x analysis/test_migration_plan.py
python3 analysis/test_migration_plan.py
```

### 4ï¸âƒ£ é¢¨éšªè©•ä¼°å’Œç·©è§£ç­–ç•¥ (1 å°æ™‚)

#### é¢¨éšªè©•ä¼°çŸ©é™£
```bash
cat > analysis/risk_assessment.py << 'EOF'
#!/usr/bin/env python3
"""
é¢¨éšªè©•ä¼°å’Œç·©è§£ç­–ç•¥ç”Ÿæˆå™¨
"""

import json

class RiskAssessment:
    def __init__(self):
        self.risks = [
            {
                "id": "RISK-001",
                "name": "æ•¸æ“šéºå¤±é¢¨éšª",
                "category": "æ•¸æ“šå®‰å…¨",
                "probability": "LOW",
                "impact": "HIGH",
                "severity": "HIGH",
                "description": "åœ¨é·ç§»éç¨‹ä¸­å¯èƒ½ä¸Ÿå¤±æ­·å²å­¸ç¿’æ•¸æ“š",
                "mitigation": [
                    "å®Œæ•´æ•¸æ“šå‚™ä»½",
                    "é›™é‡é©—è­‰æ©Ÿåˆ¶",
                    "éšæ®µæ€§é·ç§»",
                    "å¯¦æ™‚æ•¸æ“šåŒæ­¥ç›£æ§"
                ],
                "contingency": "ç«‹å³å›æ»¾åˆ°å‚™ä»½ç‰ˆæœ¬"
            },
            {
                "id": "RISK-002", 
                "name": "æ¥­å‹™ä¸­æ–·é¢¨éšª",
                "category": "æ¥­å‹™é€£çºŒæ€§",
                "probability": "MEDIUM",
                "impact": "MEDIUM",
                "severity": "MEDIUM",
                "description": "é·ç§»æœŸé–“ç”¨æˆ¶ç„¡æ³•æ­£å¸¸ä½¿ç”¨å­¸ç¿’åŠŸèƒ½",
                "mitigation": [
                    "æ»¾å‹•æ›´æ–°ç­–ç•¥",
                    "åŠŸèƒ½é–‹é—œæ§åˆ¶",
                    "ç°åº¦ç™¼å¸ƒ",
                    "å¿«é€Ÿå›æ»¾æ©Ÿåˆ¶"
                ],
                "contingency": "å•Ÿç”¨ç¶­è­·æ¨¡å¼ï¼ŒåŸ·è¡Œç·Šæ€¥å›æ»¾"
            },
            {
                "id": "RISK-003",
                "name": "æ€§èƒ½å›æ­¸é¢¨éšª", 
                "category": "ç³»çµ±æ€§èƒ½",
                "probability": "LOW",
                "impact": "MEDIUM",
                "severity": "LOW",
                "description": "è³‡æ–™åº«æŸ¥è©¢æ€§èƒ½ä¸å¦‚ JSON æ–‡ä»¶è®€å–",
                "mitigation": [
                    "è³‡æ–™åº«ç´¢å¼•å„ªåŒ–",
                    "æŸ¥è©¢æ€§èƒ½åŸºæº–æ¸¬è©¦",
                    "é€£æ¥æ± é…ç½®å„ªåŒ–",
                    "ç·©å­˜ç­–ç•¥æ”¹é€²"
                ],
                "contingency": "èª¿æ•´è³‡æ–™åº«é…ç½®ï¼Œå¢åŠ ç·©å­˜å±¤"
            },
            {
                "id": "RISK-004",
                "name": "é–‹ç™¼ç’°å¢ƒè¤‡é›œåŒ–",
                "category": "é–‹ç™¼æ•ˆç‡",
                "probability": "HIGH",
                "impact": "LOW", 
                "severity": "MEDIUM",
                "description": "é–‹ç™¼è€…éœ€è¦é‹è¡Œè³‡æ–™åº«ï¼Œå¢åŠ ç’°å¢ƒé…ç½®è¤‡é›œåº¦",
                "mitigation": [
                    "Docker å®¹å™¨åŒ–",
                    "ä¸€éµå•Ÿå‹•è…³æœ¬",
                    "é–‹ç™¼æ–‡æª”æ›´æ–°",
                    "åœ˜éšŠåŸ¹è¨“"
                ],
                "contingency": "æä¾›è©³ç´°çš„æ•…éšœæ’é™¤æŒ‡å—"
            },
            {
                "id": "RISK-005",
                "name": "æ¸¬è©¦è¦†è“‹ä¸è¶³",
                "category": "ä»£ç¢¼è³ªé‡",
                "probability": "MEDIUM",
                "impact": "HIGH",
                "severity": "HIGH",
                "description": "é·ç§»å¾Œæ¸¬è©¦è¦†è“‹ç‡ä¸‹é™ï¼Œæœªç™¼ç¾çš„ bug å¢åŠ ",
                "mitigation": [
                    "é‡æ§‹æ¸¬è©¦å¥—ä»¶",
                    "å¢åŠ é›†æˆæ¸¬è©¦",
                    "è‡ªå‹•åŒ–æ¸¬è©¦é©—è­‰",
                    "æ¸¬è©¦è¦†è“‹ç‡ç›£æ§"
                ],
                "contingency": "æš«åœç™¼å¸ƒï¼Œè£œå……æ¸¬è©¦ç”¨ä¾‹"
            }
        ]
    
    def calculate_risk_score(self, probability: str, impact: str) -> int:
        """è¨ˆç®—é¢¨éšªåˆ†æ•¸"""
        prob_score = {"LOW": 1, "MEDIUM": 2, "HIGH": 3}[probability]
        impact_score = {"LOW": 1, "MEDIUM": 2, "HIGH": 3}[impact]
        return prob_score * impact_score
    
    def generate_risk_matrix(self):
        """ç”Ÿæˆé¢¨éšªçŸ©é™£"""
        matrix = {
            "HIGH": [],
            "MEDIUM": [],
            "LOW": []
        }
        
        for risk in self.risks:
            severity = risk["severity"]
            matrix[severity].append(risk)
        
        return matrix
    
    def generate_mitigation_plan(self):
        """ç”Ÿæˆç·©è§£è¨ˆåŠƒ"""
        matrix = self.generate_risk_matrix()
        
        plan = f"""# é¢¨éšªè©•ä¼°èˆ‡ç·©è§£è¨ˆåŠƒ

## ğŸ¯ é¢¨éšªç¸½è¦½

### é¢¨éšªåˆ†å¸ƒ
- **é«˜é¢¨éšªé …ç›®**: {len(matrix['HIGH'])}
- **ä¸­é¢¨éšªé …ç›®**: {len(matrix['MEDIUM'])}
- **ä½é¢¨éšªé …ç›®**: {len(matrix['LOW'])}

## ğŸ”´ é«˜é¢¨éšªé …ç›® (ç«‹å³è™•ç†)
"""
        
        for risk in matrix['HIGH']:
            plan += f"""
### {risk['id']}: {risk['name']}
- **é¡åˆ¥**: {risk['category']}
- **ç™¼ç”Ÿæ¦‚ç‡**: {risk['probability']}
- **å½±éŸ¿ç¨‹åº¦**: {risk['impact']}
- **æè¿°**: {risk['description']}

**ç·©è§£æªæ–½**:
"""
            for mitigation in risk['mitigation']:
                plan += f"- {mitigation}\n"
            
            plan += f"\n**æ‡‰æ€¥è¨ˆåŠƒ**: {risk['contingency']}\n"
        
        plan += f"""

## ğŸŸ  ä¸­é¢¨éšªé …ç›® (é‡é»é—œæ³¨)
"""
        
        for risk in matrix['MEDIUM']:
            plan += f"""
### {risk['id']}: {risk['name']}
- **é¡åˆ¥**: {risk['category']}
- **æè¿°**: {risk['description']}
- **é—œéµç·©è§£æªæ–½**: {risk['mitigation'][0]}
"""
        
        plan += """

## ğŸŸ¡ ä½é¢¨éšªé …ç›® (ç›£æ§å³å¯)
"""
        
        for risk in matrix['LOW']:
            plan += f"- **{risk['id']}**: {risk['name']} - {risk['mitigation'][0]}\n"
        
        plan += """

## ğŸ“‹ é¢¨éšªç›£æ§æª¢æŸ¥æ¸…å–®

### é·ç§»å‰æª¢æŸ¥
- [ ] å®Œæ•´æ•¸æ“šå‚™ä»½å·²å‰µå»ºä¸¦é©—è­‰
- [ ] å›æ»¾è…³æœ¬å·²æ¸¬è©¦
- [ ] åœ˜éšŠå·²æ¥å—åŸ¹è¨“
- [ ] ç›£æ§ç³»çµ±å·²å°±ç·’

### é·ç§»éç¨‹ç›£æ§
- [ ] å¯¦æ™‚éŒ¯èª¤ç‡ç›£æ§ (<1%)
- [ ] éŸ¿æ‡‰æ™‚é–“ç›£æ§ (<200ms)
- [ ] æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ (æ¯å°æ™‚)
- [ ] ç”¨æˆ¶åé¦ˆç›£æ§

### é·ç§»å¾Œé©—è­‰
- [ ] åŠŸèƒ½æ¸¬è©¦é€šéç‡ >99%
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦é€šé
- [ ] æ•¸æ“šä¸€è‡´æ€§é©—è­‰
- [ ] ç”¨æˆ¶é©—æ”¶æ¸¬è©¦é€šé

## ğŸš¨ ç·Šæ€¥éŸ¿æ‡‰ç¨‹åº

### è§¸ç™¼æ¢ä»¶
1. éŒ¯èª¤ç‡ >5%
2. éŸ¿æ‡‰æ™‚é–“ >500ms
3. æ•¸æ“šä¸ä¸€è‡´
4. ç”¨æˆ¶ç„¡æ³•è¨ªå•

### éŸ¿æ‡‰æ­¥é©Ÿ
1. **ç«‹å³**: å•Ÿå‹•ç·Šæ€¥æœƒè­°
2. **5åˆ†é˜å…§**: è©•ä¼°å•é¡Œåš´é‡æ€§
3. **10åˆ†é˜å…§**: æ±ºå®šæ˜¯å¦å›æ»¾
4. **15åˆ†é˜å…§**: åŸ·è¡Œå›æ»¾æˆ–ä¿®å¾©
5. **äº‹å¾Œ**: é€²è¡Œäº‹å¾Œåˆ†æå’Œæ”¹é€²
"""
        
        return plan

def main():
    assessment = RiskAssessment()
    plan = assessment.generate_mitigation_plan()
    
    with open('analysis/risk_assessment_and_mitigation.md', 'w') as f:
        f.write(plan)
    
    # ä¿å­˜çµæ§‹åŒ–é¢¨éšªæ•¸æ“š
    with open('analysis/risk_data.json', 'w') as f:
        json.dump(assessment.risks, f, indent=2)
    
    print("âœ… é¢¨éšªè©•ä¼°å®Œæˆï¼")
    print("ğŸ“„ å ±å‘Š: analysis/risk_assessment_and_mitigation.md")
    print("ğŸ“Š æ•¸æ“š: analysis/risk_data.json")

if __name__ == "__main__":
    main()
EOF

chmod +x analysis/risk_assessment.py
python3 analysis/risk_assessment.py
```

## ğŸ”§ åŸ·è¡Œé©—è­‰

### åˆ†æçµæœé©—è­‰
```bash
# æª¢æŸ¥æ‰€æœ‰åˆ†ææ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
echo "=== Analysis Files Generated ===" 
ls -la analysis/

# é©—è­‰åˆ†æçµæœçš„å®Œæ•´æ€§
echo "=== Dependency Analysis Validation ==="
if [[ -f "analysis/detailed_dependency_analysis.json" ]]; then
    python3 -c "
import json
with open('analysis/detailed_dependency_analysis.json') as f:
    data = json.load(f)
print(f'âœ… JSON ä¾è³´åˆ†æ: {data[\"summary\"][\"json_related_files\"]} å€‹ç›¸é—œæ–‡ä»¶')
print(f'âœ… é«˜é¢¨éšªæ–‡ä»¶: {data[\"summary\"][\"high_risk_files\"]} å€‹')
"
else
    echo "âŒ ä¾è³´åˆ†ææ–‡ä»¶ç¼ºå¤±"
fi

# é©—è­‰æ¥­å‹™å½±éŸ¿åˆ†æ
if [[ -f "analysis/business_impact_analysis.md" ]]; then
    echo "âœ… æ¥­å‹™å½±éŸ¿åˆ†æå®Œæˆ"
    grep -c "é¢¨éšªç­‰ç´š" analysis/business_impact_analysis.md
else
    echo "âŒ æ¥­å‹™å½±éŸ¿åˆ†æç¼ºå¤±"
fi

# é©—è­‰æ¸¬è©¦é·ç§»è¨ˆåŠƒ
if [[ -f "analysis/test_migration_plan.md" ]]; then
    echo "âœ… æ¸¬è©¦é·ç§»è¨ˆåŠƒå®Œæˆ"
    grep -c "è¤‡é›œåº¦" analysis/test_migration_plan.md
else
    echo "âŒ æ¸¬è©¦é·ç§»è¨ˆåŠƒç¼ºå¤±"
fi

# é©—è­‰é¢¨éšªè©•ä¼°
if [[ -f "analysis/risk_assessment_and_mitigation.md" ]]; then
    echo "âœ… é¢¨éšªè©•ä¼°å®Œæˆ"
    grep -c "RISK-" analysis/risk_assessment_and_mitigation.md
else
    echo "âŒ é¢¨éšªè©•ä¼°ç¼ºå¤±"
fi
```

### ç”Ÿæˆæœ€çµ‚æ‘˜è¦å ±å‘Š
```bash
cat > analysis/FINAL_ANALYSIS_SUMMARY.md << 'EOF'
# ç´” Database ç³»çµ±é‡æ§‹ - è©³ç´°ä¾è³´åˆ†ææœ€çµ‚å ±å‘Š

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

**åˆ†æå®Œæˆæ™‚é–“**: $(date)
**åˆ†æç¯„åœ**: æ•´å€‹ Linker å°ˆæ¡ˆä»£ç¢¼åº«
**åˆ†ææ·±åº¦**: ä»£ç¢¼ç´šåˆ¥ä¾è³´ + æ¥­å‹™æµç¨‹ + é¢¨éšªè©•ä¼°

## ğŸ” é—œéµç™¼ç¾

### æŠ€è¡“ç™¼ç¾
1. **JSON ä¾è³´æ–‡ä»¶**: å·²è­˜åˆ¥æ‰€æœ‰å« JSON é‚è¼¯çš„æ–‡ä»¶
2. **é¢¨éšªç­‰ç´šåˆ†å¸ƒ**: é«˜/ä¸­/ä½é¢¨éšªæ–‡ä»¶åˆ†é¡å®Œæˆ
3. **èª¿ç”¨éˆåˆ†æ**: å®Œæ•´çš„å‡½æ•¸èª¿ç”¨é—œä¿‚åœ–
4. **æ¸¬è©¦ä¾è³´**: æ¸¬è©¦ç³»çµ±é·ç§»è¨ˆåŠƒå·²åˆ¶å®š

### æ¥­å‹™å½±éŸ¿
1. **æ ¸å¿ƒæµç¨‹**: æ‰€æœ‰é—œéµæ¥­å‹™æµç¨‹éƒ½æœ‰è³‡æ–™åº«æ›¿ä»£æ–¹æ¡ˆ
2. **API ç«¯é»**: å—å½±éŸ¿çš„ç«¯é»å·²æ¨™è¨˜å’Œåˆ†é¡
3. **æ•¸æ“šæµå‘**: å®Œæ•´çš„æ•¸æ“šæµè½‰æ›è¨ˆåŠƒ
4. **ç”¨æˆ¶é«”é©—**: å°çµ‚ç«¯ç”¨æˆ¶é€æ˜çš„é·ç§»ç­–ç•¥

### é¢¨éšªæ§åˆ¶
1. **æ•¸æ“šå®‰å…¨**: å¤šå±¤å‚™ä»½å’Œé©—è­‰æ©Ÿåˆ¶
2. **æ¥­å‹™é€£çºŒæ€§**: æ¼¸é€²å¼é·ç§»å’Œå¿«é€Ÿå›æ»¾
3. **è³ªé‡ä¿è­‰**: å…¨é¢çš„æ¸¬è©¦è¦†è“‹å’Œç›£æ§
4. **åœ˜éšŠæº–å‚™**: å®Œæ•´çš„åŸ¹è¨“å’Œæ–‡æª”

## ğŸ“‹ å¾ŒçºŒè¡Œå‹•è¨ˆåŠƒ

### ç«‹å³è¡Œå‹• (TASK-30C)
- [ ] åŸºæ–¼åˆ†æçµæœé…ç½®é–‹ç™¼ç’°å¢ƒ
- [ ] å‰µå»ºé–‹ç™¼å·¥å…·å’Œè…³æœ¬
- [ ] æº–å‚™åœ˜éšŠåŸ¹è¨“ææ–™

### ç¬¬ä¸€éšæ®µå¯¦æ–½ (TASK-30D~H)
- [ ] æŒ‰é¢¨éšªç­‰ç´šé †åºç§»é™¤ JSON ä¾è³´
- [ ] å¯¦æ–½æ¼¸é€²å¼åˆ‡æ›ç­–ç•¥
- [ ] æŒçºŒç›£æ§å’Œé©—è­‰

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

- **ä»£ç¢¼è³ªé‡**: ç„¡ JSON å¼•ç”¨ï¼ŒLinting é€šé
- **æ¸¬è©¦è¦†è“‹**: ç¶­æŒ >90% è¦†è“‹ç‡
- **æ€§èƒ½æŒ‡æ¨™**: éŸ¿æ‡‰æ™‚é–“ä¸é™ä½
- **æ•¸æ“šå®Œæ•´æ€§**: 100% æ•¸æ“šä¸€è‡´æ€§
- **ç”¨æˆ¶æ»¿æ„åº¦**: ç„¡åŠŸèƒ½å›æ­¸

---

**âœ… åˆ†æéšæ®µå®Œæˆ**: æ‰€æœ‰æŠ€è¡“å’Œæ¥­å‹™åˆ†æå·²å®Œæˆï¼Œå¯ä»¥é€²å…¥å¯¦æ–½éšæ®µã€‚
EOF
```

## ğŸ“ åŸ·è¡Œç­†è¨˜

### å®Œæˆæª¢æŸ¥æ¸…å–®
- [ ] å®Œæ•´ä»£ç¢¼æƒæå’Œä¾è³´ç™¼ç¾
- [ ] æ·±åº¦ä¾è³´åˆ†æè…³æœ¬åŸ·è¡Œ
- [ ] æ¥­å‹™æµç¨‹å½±éŸ¿è©•ä¼°
- [ ] API ç«¯é»å½±éŸ¿åˆ†æ
- [ ] æ¸¬è©¦ç³»çµ±é·ç§»è¨ˆåŠƒ
- [ ] é¢¨éšªè©•ä¼°çŸ©é™£å®Œæˆ
- [ ] ç·©è§£ç­–ç•¥åˆ¶å®š
- [ ] æœ€çµ‚æ‘˜è¦å ±å‘Šç”Ÿæˆ

### é‡è¦è¼¸å‡ºæ–‡ä»¶
- `analysis/detailed_dependency_analysis.json` - çµæ§‹åŒ–ä¾è³´æ•¸æ“š
- `analysis/dependency_analysis_summary.md` - ä¾è³´åˆ†ææ‘˜è¦
- `analysis/business_impact_analysis.md` - æ¥­å‹™å½±éŸ¿è©•ä¼°
- `analysis/test_migration_plan.md` - æ¸¬è©¦é·ç§»è¨ˆåŠƒ
- `analysis/risk_assessment_and_mitigation.md` - é¢¨éšªè©•ä¼°å’Œç·©è§£
- `analysis/FINAL_ANALYSIS_SUMMARY.md` - æœ€çµ‚æ‘˜è¦å ±å‘Š

### åˆ†æå“è³ªæŒ‡æ¨™
- **è¦†è“‹å®Œæ•´æ€§**: 100% ä»£ç¢¼æ–‡ä»¶æƒæ
- **é¢¨éšªè­˜åˆ¥**: 5å€‹ä¸»è¦é¢¨éšªé¡åˆ¥
- **æ¥­å‹™æµç¨‹**: 6å€‹æ ¸å¿ƒæ¥­å‹™æµç¨‹åˆ†æ
- **æŠ€è¡“æ·±åº¦**: å‡½æ•¸ç´šåˆ¥çš„èª¿ç”¨éˆåˆ†æ

## ğŸ” å¯©æŸ¥æ„è¦‹ (For Reviewer)

_(ç•™ç©ºï¼Œä¾› reviewer å¡«å¯«)_

---

**âœ… ä»»å‹™å®Œæˆæ¨™æº–**: æ‰€æœ‰åˆ†æè…³æœ¬åŸ·è¡ŒæˆåŠŸï¼Œç”Ÿæˆå®Œæ•´çš„æŠ€è¡“å’Œæ¥­å‹™åˆ†æå ±å‘Šï¼Œé¢¨éšªè©•ä¼°å’Œç·©è§£è¨ˆåŠƒåˆ¶å®šå®Œæˆï¼Œç‚ºå¾ŒçºŒå¯¦æ–½æä¾›è©³ç´°çš„æŠ€è¡“è·¯ç·šåœ–ã€‚