"""
AI æœå‹™æ¨¡çµ„

è² è²¬è™•ç†èˆ‡ Google Gemini API çš„æ‰€æœ‰äº’å‹•ï¼ŒåŒ…æ‹¬ï¼š
- åˆå§‹åŒ– Gemini æ¨¡å‹
- å°è£æ¨¡å‹èª¿ç”¨é‚è¼¯
- æ‰¹æ”¹ä½¿ç”¨è€…ç¿»è­¯
- ç”Ÿæˆæ–°çš„ç·´ç¿’é¡Œ
- ç”Ÿæˆè¤‡ç¿’é¡Œ
- æ ¹æ“šæ¨™ç±¤ç”Ÿæˆé¡Œç›®
- åˆ†æå¸¸è¦‹éŒ¯èª¤
"""

import asyncio
import contextlib
import json
import os
import time
from typing import Any, Optional

from core.log_config import get_module_logger


class AIService:
    """
    AI æœå‹™é¡ï¼Œå°è£äº†å° Gemini API çš„èª¿ç”¨ã€‚

    æä¾›å…©ç¨®æ¨¡å‹ï¼š
    - generate_model: ç”¨æ–¼ç”Ÿæˆé¡Œç›®ï¼Œè¨­å®šè¼ƒé«˜çš„ temperature ä»¥å¢åŠ å‰µæ„ã€‚
    - grade_model: ç”¨æ–¼æ‰¹æ”¹ï¼Œè¨­å®šè¼ƒä½çš„ temperature ä»¥ç¢ºä¿ä¸€è‡´æ€§å’Œæº–ç¢ºæ€§ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ– AI æœå‹™ï¼Œè¼‰å…¥ API é‡‘é‘°ä¸¦è¨­å®šæ¨¡å‹ã€‚"""
        self.logger = get_module_logger(__name__)
        self.api_key = os.getenv("GEMINI_API_KEY")
        # æ¨¡å‹åç¨±å¯ç”±ç’°å¢ƒè®Šæ•¸è¦†è“‹ï¼Œæä¾›éˆæ´»æ€§
        self.generate_model_name = os.getenv("GEMINI_GENERATE_MODEL", "gemini-2.5-flash")
        self.grade_model_name = os.getenv("GEMINI_GRADE_MODEL", "gemini-2.5-pro")
        self.generate_model = None
        self.grade_model = None
        self._init_gemini()
        # å„²å­˜æœ€è¿‘çš„ LLM äº’å‹•è¨˜éŒ„ï¼Œæ–¹ä¾¿èª¿è©¦
        self.last_llm_interaction = None

    def _init_gemini(self):
        """
        åˆå§‹åŒ– Gemini APIã€‚
        å¦‚æœæ²’æœ‰ API é‡‘é‘°ï¼Œå°‡ä»¥ fallback æ¨¡å¼é‹è¡Œã€‚
        """
        try:
            if not self.api_key:
                self.logger.warning(
                    "GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®ï¼ŒAI æœå‹™å°‡ä»¥ fallback æ¨¡å¼é‹è¡Œã€‚"
                )
                return

            import google.generativeai as genai

            genai.configure(api_key=self.api_key)
            # å‡ºé¡Œæ¨¡å‹ï¼ˆå¿«é€Ÿä¸”æœ‰å‰µæ„ï¼‰
            self.generate_model = genai.GenerativeModel(
                self.generate_model_name,
                generation_config=genai.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=1.0,  # æé«˜æº«åº¦ä»¥å¢åŠ å‰µæ„å’Œè®ŠåŒ–
                    top_p=0.95,
                    top_k=40,
                ),
            )
            # æ‰¹æ”¹æ¨¡å‹ï¼ˆé«˜å“è³ªä¸”ç©©å®šï¼‰
            self.grade_model = genai.GenerativeModel(
                self.grade_model_name,
                generation_config=genai.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=0.2,  # è¼ƒä½æº«åº¦ç¢ºä¿æ‰¹æ”¹ä¸€è‡´æ€§
                    top_p=0.9,
                ),
            )
            self.logger.info("Gemini API åˆå§‹åŒ–æˆåŠŸã€‚")
        except ImportError:
            self.logger.error(
                "ç¼ºå°‘ google-generativeai å¥—ä»¶ï¼Œè«‹åŸ·è¡Œ: pip install google-generativeai"
            )
            raise
        except Exception as e:
            self.logger.error(f"Gemini åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def _call_model(
        self, model, system_prompt: str, user_prompt: str, use_cache: bool = True, timeout: int = 30
    ) -> dict[str, Any]:
        """
        å…§éƒ¨æ–¹æ³•ï¼Œç”¨æ–¼èª¿ç”¨æŒ‡å®šçš„ Gemini æ¨¡å‹ã€‚

        Args:
            model: è¦èª¿ç”¨çš„ Gemini æ¨¡å‹å¯¦ä¾‹ã€‚
            system_prompt: ç³»çµ±æç¤ºè©ã€‚
            user_prompt: ä½¿ç”¨è€…æç¤ºè©ã€‚
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–ï¼ˆç›®å‰å·²ç¦ç”¨ï¼‰ã€‚
            timeout: API èª¿ç”¨è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 30 ç§’ã€‚

        Returns:
            ä¸€å€‹åŒ…å«æ¨¡å‹å›æ‡‰çš„å­—å…¸ã€‚
        """
        start_time = time.time()

        try:
            import google.generativeai as genai
            from google.api_core import retry
            
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            # ğŸ”§ ä¿®å¾©ï¼šå®‰å…¨åœ°ç²å–æ¨¡å‹é…ç½®ä¿¡æ¯
            model_config = {
                "model_name": getattr(model, "model_name", str(model)),
                "temperature": getattr(getattr(model, "generation_config", None), "temperature", None),
                "top_p": getattr(getattr(model, "generation_config", None), "top_p", None),
                "top_k": getattr(getattr(model, "generation_config", None), "top_k", None),
            }

            # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ è¶…æ™‚å’Œé‡è©¦æ©Ÿåˆ¶
            request_options = genai.types.RequestOptions(
                timeout=timeout,  # è¨­ç½®è¶…æ™‚æ™‚é–“
                retry=retry.Retry(
                    initial=1.0,        # åˆå§‹é‡è©¦å»¶é² 1 ç§’
                    maximum=3.0,        # æœ€å¤§é‡è©¦å»¶é² 3 ç§’  
                    multiplier=1.5,     # é‡è©¦å»¶é²å€å¢ä¿‚æ•¸
                    deadline=timeout,   # ç¸½é«”è¶…æ™‚æ™‚é–“
                    predicate=retry.if_transient_error  # åªé‡è©¦æš«æ™‚æ€§éŒ¯èª¤
                )
            )

            response = model.generate_content(full_prompt, request_options=request_options)
            duration_ms = int((time.time() - start_time) * 1000)
            result = self._parse_response(response.text)

            # è¨˜éŒ„è©³ç´°çš„äº’å‹•è³‡è¨Šä»¥ä¾›èª¿è©¦
            self.last_llm_interaction = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_ms": duration_ms,
                "model_config": model_config,
                "input": {
                    "system_prompt": system_prompt,
                    "user_prompt": user_prompt,
                    "full_prompt": full_prompt,
                    "prompt_length": len(full_prompt),
                },
                "output": {
                    "raw_response": response.text,
                    "parsed_result": result,
                    "response_length": len(response.text),
                },
                "status": "success",
            }

            # è¨˜éŒ„ API èª¿ç”¨æ—¥èªŒ
            with contextlib.suppress(Exception):
                self.logger.log_api_call(
                    api_name="gemini",
                    method="generate_content",
                    params={"prompt_preview": full_prompt[:200]},
                    response={k: result.get(k) for k in list(result.keys())[:3]}
                    if isinstance(result, dict)
                    else None,
                )

            return result

        except Exception as e:
            # è¨˜éŒ„å¤±æ•—çš„äº’å‹•è³‡è¨Š
            self.last_llm_interaction = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_ms": int((time.time() - start_time) * 1000),
                "model_config": locals().get("model_config", {}),
                "input": {
                    "system_prompt": system_prompt,
                    "user_prompt": user_prompt,
                    "full_prompt": f"{system_prompt}\n\n{user_prompt}",
                    "prompt_length": len(f"{system_prompt}\n\n{user_prompt}"),
                },
                "output": {"error": str(e), "error_type": type(e).__name__},
                "status": "error",
            }
            self.logger.log_api_call(
                api_name="gemini",
                method="generate_content",
                params={"prompt_preview": (system_prompt + user_prompt)[:200]},
                error=e,
            )
            return self._get_fallback_response()

    def _parse_response(self, text: str) -> dict[str, Any]:
        """
        è§£ææ¨¡å‹çš„ JSON å›æ‡‰ã€‚
        å¦‚æœç›´æ¥è§£æå¤±æ•—ï¼Œæœƒå˜—è©¦å¾æ–‡æœ¬ä¸­æå– JSON çµæ§‹ã€‚

        Args:
            text: æ¨¡å‹è¿”å›çš„åŸå§‹æ–‡æœ¬ã€‚

        Returns:
            è§£æå¾Œçš„å­—å…¸ã€‚

        Raises:
            ValueError: å¦‚æœç„¡æ³•è§£æ JSONã€‚
        """
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            import re

            match = re.search(r"\{{.*\}}", text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group())
                except json.JSONDecodeError:
                    pass  # å¦‚æœæå–çš„éƒ¨åˆ†ä»ç„¶ç„¡æ³•è§£æï¼Œå‰‡æ‹‹å‡ºåŸå§‹éŒ¯èª¤
            raise ValueError("ç„¡æ³•å¾ AI å›æ‡‰ä¸­è§£æ JSON") from e

    def _get_fallback_response(self) -> dict[str, Any]:
        """
        åœ¨ AI æœå‹™å¤±æ•—æ™‚ï¼Œæä¾›ä¸€å€‹çµ±ä¸€çš„å‚™ç”¨å›æ‡‰ã€‚

        Returns:
            ä¸€å€‹åŒ…å«éŒ¯èª¤æ¨™è¨˜çš„å­—å…¸ã€‚
        """
        return {
            "is_generally_correct": False,
            "overall_suggestion": "AI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
            "error_analysis": [],
            "service_error": True,  # æ–°å¢æ¨™è¨˜ï¼Œæ–¹ä¾¿å¾Œç«¯è­˜åˆ¥æœå‹™éŒ¯èª¤
        }

    def health_check(self) -> dict[str, Any]:
        """
        ğŸ”¥ é€æ˜åŒ–æ”¹é€ ï¼šAI æœå‹™å¥åº·æª¢æŸ¥
        
        çœŸå¯¦æª¢æ¸¬ AI æœå‹™çš„å¯ç”¨æ€§ï¼Œä¸èªªè¬Šï¼
        
        Returns:
            åŒ…å«æœå‹™å¥åº·ç‹€æ³çš„è©³ç´°å ±å‘Š
        """
        health_status = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "service_name": "AI Service (Gemini)",
            "status": "unknown",
            "details": {},
            "last_error": None,
            "recommendations": []
        }
        
        try:
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
            if not self.generate_model:
                health_status.update({
                    "status": "unavailable",
                    "details": {"reason": "generate_model_not_initialized"},
                    "last_error": "Gemini API æœªæ­£ç¢ºåˆå§‹åŒ–",
                    "recommendations": ["æª¢æŸ¥ GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸", "é‡å•Ÿæœå‹™"]
                })
                return health_status
            
            # é€²è¡Œå¯¦éš›çš„ API æ¸¬è©¦èª¿ç”¨
            test_prompt = "ç°¡å–®æ¸¬è©¦ï¼šå›ç­” 'OK'"
            start_time = time.time()
            
            # ğŸ”§ ä¿®å¾©ï¼šå¥åº·æª¢æŸ¥ä½¿ç”¨è¼ƒçŸ­çš„è¶…æ™‚æ™‚é–“
            import google.generativeai as genai
            from google.api_core import retry
            
            request_options = genai.types.RequestOptions(
                timeout=10,  # å¥åº·æª¢æŸ¥åªçµ¦ 10 ç§’è¶…æ™‚
                retry=retry.Retry(
                    initial=0.5,
                    maximum=2.0,
                    multiplier=1.5,
                    deadline=10,
                    predicate=retry.if_transient_error
                )
            )
            
            response = self.generate_model.generate_content(test_prompt, request_options=request_options)
            duration_ms = int((time.time() - start_time) * 1000)
            
            if response and response.text:
                health_status.update({
                    "status": "healthy",
                    "details": {
                        "response_time_ms": duration_ms,
                        "model_accessible": True,
                        "api_responsive": True,
                        "test_response_length": len(response.text)
                    },
                    "recommendations": ["æœå‹™é‹è¡Œæ­£å¸¸"] if duration_ms < 5000 else ["éŸ¿æ‡‰æ™‚é–“è¼ƒæ…¢ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥"]
                })
            else:
                health_status.update({
                    "status": "degraded",
                    "details": {"reason": "empty_response"},
                    "last_error": "API å›æ‡‰ç‚ºç©º",
                    "recommendations": ["æª¢æŸ¥ API é…é¡", "æª¢æŸ¥ç¶²è·¯é€£æ¥"]
                })
                
        except Exception as e:
            health_status.update({
                "status": "failed",
                "details": {"reason": "api_call_failed", "error_type": type(e).__name__},
                "last_error": str(e),
                "recommendations": ["æª¢æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆ", "æª¢æŸ¥ç¶²è·¯é€£æ¥", "æª¢æŸ¥ API é…é¡"]
            })
            
        return health_status

    async def generate_async(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 3000,
        model: Optional[Any] = None,
    ) -> str:
        """
        ç•°æ­¥ç”Ÿæˆå…§å®¹ã€‚
        æ­¤æ–¹æ³•ä½¿ç”¨åŸ·è¡Œç·’æ± åœ¨èƒŒæ™¯åŸ·è¡ŒåŒæ­¥çš„ `generate_content` æ–¹æ³•ï¼Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç’°ã€‚

        Args:
            prompt: å®Œæ•´çš„æç¤ºè©ã€‚
            temperature: ç”Ÿæˆçš„æº«åº¦ã€‚
            max_tokens: æœ€å¤§ token æ•¸ã€‚
            model: è¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚æœæœªæä¾›ï¼Œå‰‡ä½¿ç”¨é è¨­çš„ç”Ÿæˆæ¨¡å‹ã€‚

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å…§å®¹ã€‚
        """
        loop = asyncio.get_event_loop()
        target_model = model or self.generate_model

        def _generate():
            try:
                import google.generativeai as genai

                target_model.generation_config = genai.GenerationConfig(
                    response_mime_type="application/json",
                    temperature=temperature,
                    max_output_tokens=max_tokens,
                    top_p=0.95,
                    top_k=40,
                )
                response = target_model.generate_content(prompt)
                return response.text
            except Exception as e:
                self.logger.error(f"ç•°æ­¥ç”Ÿæˆå¤±æ•—: {e}")
                raise

        return await loop.run_in_executor(None, _generate)

    def grade_translation(
        self, chinese: str, english: str, hint: Optional[str] = None
    ) -> dict[str, Any]:
        """
        æ‰¹æ”¹å­¸ç”Ÿçš„ç¿»è­¯ã€‚
        ä½¿ç”¨è©³ç´°çš„ç³»çµ±æç¤ºè©æŒ‡å°æ¨¡å‹é€²è¡Œå¤šç¶­åº¦åˆ†æï¼Œä¸¦ä»¥çµæ§‹åŒ–çš„ JSON æ ¼å¼å›å‚³ã€‚

        Args:
            chinese: ä¸­æ–‡åŸå¥ã€‚
            english: å­¸ç”Ÿçš„è‹±æ–‡ç¿»è­¯ã€‚
            hint: ç¿»è­¯æç¤ºã€‚

        Returns:
            åŒ…å«æ‰¹æ”¹çµæœçš„å­—å…¸ã€‚
        """
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡æ•™å¸«ï¼Œè«‹æ‰¹æ”¹å­¸ç”Ÿçš„ç¿»è­¯ã€‚

        ä¸­æ–‡åŸå¥ï¼š{chinese}
        {"æç¤ºï¼š" + hint if hint else ""}

        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

        1. is_generally_correct (boolean): ç¿»è­¯æ˜¯å¦åŸºæœ¬æ­£ç¢ºã€‚

        2. overall_suggestion (string): å»ºè­°çš„æœ€ä½³ç¿»è­¯ï¼ˆå®Œæ•´å¥å­ï¼Œä½¿ç”¨è‹±æ–‡ï¼‰ã€‚

        3. error_analysis (array): éŒ¯èª¤åˆ†æåˆ—è¡¨ï¼Œæ¯å€‹éŒ¯èª¤åŒ…å«ï¼š
           - category (string): éŒ¯èª¤åˆ†é¡ä»£ç¢¼ï¼Œå¿…é ˆæ˜¯ä»¥ä¸‹å…¶ä¸­ä¸€ç¨®ï¼š
             * \"systematic\" - ç³»çµ±æ€§éŒ¯èª¤ï¼šæ¶‰åŠæ–‡æ³•è¦å‰‡ï¼Œå­¸æœƒè¦å‰‡å¾Œå¯é¿å…åŒé¡éŒ¯èª¤ï¼ˆå¦‚æ™‚æ…‹ã€ä¸»è¬‚ä¸€è‡´ã€å‹•è©è®ŠåŒ–ï¼‰ã€‚
             * \"isolated\" - å–®ä¸€æ€§éŒ¯èª¤ï¼šéœ€è¦å€‹åˆ¥è¨˜æ†¶çš„å…§å®¹ï¼ˆå¦‚ç‰¹å®šè©å½™ã€æ­é…è©ã€ä»‹ä¿‚è©ã€æ‹¼å¯«ï¼‰ã€‚
             * \"enhancement\" - å¯ä»¥æ›´å¥½ï¼šæ–‡æ³•æ­£ç¢ºä½†è¡¨é”å¯ä»¥æ›´è‡ªç„¶ã€æ›´é“åœ°ã€‚
             * \"other\" - å…¶ä»–éŒ¯èª¤ï¼šä¸å±¬æ–¼ä¸Šè¿°é¡åˆ¥çš„éŒ¯èª¤ï¼ˆå¦‚æ¼è­¯ã€ç†è§£éŒ¯èª¤ï¼‰ã€‚

           - key_point_summary (string): éŒ¯èª¤é‡é»çš„å…·é«”æè¿°ï¼Œæ ¼å¼ç‚ºã€ŒéŒ¯èª¤é¡å‹: å…·é«”éŒ¯èª¤å…§å®¹ã€ã€‚
             ä¾‹å¦‚ï¼šã€Œå–®å­—æ‹¼å¯«éŒ¯èª¤: irrevertableã€ã€ã€Œæ™‚æ…‹éŒ¯èª¤: have â†’ hasã€ã€ã€Œä»‹ç³»è©æ­é…: in â†’ onã€ã€‚
             é€™æ¨£å¯ä»¥å€åˆ†ä¸åŒçš„å…·é«”éŒ¯èª¤ï¼Œé¿å…æŠŠä¸ç›¸é—œçš„éŒ¯èª¤æ­¸åœ¨ä¸€èµ·ï¼ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ã€‚

           - original_phrase (string): å­¸ç”Ÿå¯«éŒ¯çš„ç‰‡èªæˆ–å¥å­éƒ¨åˆ†ã€‚

           - correction (string): æ­£ç¢ºçš„å¯«æ³•ã€‚

           - explanation (string): è©³ç´°è§£é‡‹ç‚ºä»€éº¼éŒ¯äº†ï¼Œä»¥åŠæ­£ç¢ºçš„ç”¨æ³•ï¼ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ã€‚

           - severity (string): åš´é‡ç¨‹åº¦ï¼Œ"major"ï¼ˆé‡è¦éŒ¯èª¤ï¼‰æˆ– "minor"ï¼ˆæ¬¡è¦å•é¡Œï¼‰ã€‚

        åˆ†é¡åŸå‰‡ï¼š
        - å¦‚æœéŒ¯èª¤æ¶‰åŠå¯ä»¥é€šéå­¸ç¿’è¦å‰‡è§£æ±ºçš„æ–‡æ³•å•é¡Œï¼Œä½¿ç”¨ \"systematic\"ã€‚
        - å¦‚æœéŒ¯èª¤éœ€è¦è¨˜æ†¶ç‰¹å®šç”¨æ³•æˆ–å–®å­—ï¼Œä½¿ç”¨ \"isolated\"ã€‚
        - å¦‚æœç¿»è­¯æ­£ç¢ºä½†å¯ä»¥æ›´è‡ªç„¶ï¼Œä½¿ç”¨ \"enhancement\"ï¼ˆé€šå¸¸æ˜¯ minorï¼‰ã€‚
        - å…¶ä»–æƒ…æ³ä½¿ç”¨ \"other\"ã€‚

        é‡è¦ï¼š
        - è«‹ç¢ºä¿ category æ¬„ä½å®Œå…¨åŒ¹é…ä¸Šè¿°å››å€‹ä»£ç¢¼ä¹‹ä¸€ã€‚
        - æ¯å€‹éŒ¯èª¤éƒ½è¦æœ‰æ¸…æ¥šçš„ explanationã€‚
        - overall_suggestion å¿…é ˆæ˜¯å®Œæ•´ã€æ­£ç¢ºçš„è‹±æ–‡å¥å­ã€‚
        """
        user_prompt = f"å­¸ç”Ÿçš„ç¿»è­¯ï¼šã€Œ{english}ã€"

        if not self.grade_model:
            return self._get_fallback_response()

        result = self._call_model(self.grade_model, system_prompt, user_prompt, timeout=20)

        # ç¢ºä¿è¿”å›çš„çµæœæ˜¯å­—å…¸
        if not isinstance(result, dict):
            return self._get_fallback_response()

        return result

    def generate_practice_sentence(
        self,
        level: int = 1,
        length: str = "short",
        examples: Optional[list[str]] = None,
        shuffle: bool = False,
    ) -> dict[str, str]:
        """
        ç”Ÿæˆæ–°çš„ç·´ç¿’å¥å­ã€‚
        ä½¿ç”¨ few-shot contextï¼ˆexamplesï¼‰å’Œè©³ç´°çš„æç¤ºè©ä¾†å¼•å°æ¨¡å‹ç”Ÿæˆé«˜å“è³ªçš„é¡Œç›®ã€‚

        Args:
            level: é›£åº¦ç­‰ç´š (1-5)ã€‚
            length: å¥å­é•·åº¦ ("short", "medium", "long")ã€‚
            examples: ç”¨æ–¼ few-shot learning çš„ä¾‹å¥ã€‚
            shuffle: æ˜¯å¦å¼·åˆ¶ç”Ÿæˆæ–°å¥å­ï¼ˆç›®å‰é è¨­å¼·åˆ¶ï¼‰ã€‚

        Returns:
            åŒ…å«ä¸­æ–‡å¥å­ã€æç¤ºã€é›£åº¦ç­‰ç´šç­‰è³‡è¨Šçš„å­—å…¸ã€‚
        """
        difficulty_map = {
            1: "åœ‹ä¸­åŸºç¤ç¨‹åº¦ï¼Œç°¡å–®è©å½™å’ŒåŸºæœ¬å¥å‹",
            2: "é«˜ä¸­ç¨‹åº¦ï¼ŒåŒ…å«å¸¸è¦‹ç‰‡èªå’Œè¤‡é›œå¥å‹",
            3: "å­¸æ¸¬ç¨‹åº¦ï¼ŒåŒ…å«é€²éšè©å½™å’Œè¤‡é›œèªæ³•",
            4: "æŒ‡è€ƒç¨‹åº¦ï¼ŒåŒ…å«é«˜éšè©å½™å’Œè¤‡é›œçµæ§‹",
            5: "é€²éšç¨‹åº¦ï¼ŒåŒ…å«å­¸è¡“æˆ–å°ˆæ¥­ç”¨èª",
        }
        length_hint = {
            "short": "å­—æ•¸ç´„10-20ï¼Œå¥å­ç°¡æ½”",
            "medium": "å­—æ•¸ç´„20-35ï¼Œçµæ§‹å®Œæ•´",
            "long": "å­—æ•¸ç´„35-60ï¼ŒåŒ…å«å¾å±¬å­å¥æˆ–åˆ†è©æ§‹å¥",
        }.get(length, "å­—æ•¸é©ä¸­")

        examples_text = "\n".join(f"- {s}" for s in (examples or [])[:5])

        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡å‡ºé¡Œæ•™å¸«ï¼Œ
        è«‹ç”Ÿæˆä¸€å€‹{difficulty_map.get(level, difficulty_map[1])}ã€ä¸”{length_hint}çš„ä¸­æ–‡å¥å­ï¼Œ
        ç”¨æ–¼è‹±æ–‡ç¿»è­¯ç·´ç¿’ã€‚

        è¦æ±‚ï¼š
        1. å¥å­è¦è‡ªç„¶ã€å¯¦ç”¨ã€ç¬¦åˆæ—¥å¸¸å°è©±æˆ–å¯«ä½œæƒ…å¢ƒã€‚
        2. é•·åº¦é©ä¸­ï¼ˆ10-30å­—ï¼‰ã€‚
        3. åŒ…å«æ˜ç¢ºçš„èªæ³•é‡é»æˆ–å¸¸è¦‹è¡¨é”ã€‚
        4. é¿å…éæ–¼æ–‡è¨€æˆ–ç½•è¦‹çš„è¡¨é”æ–¹å¼ã€‚
        5. è«‹åƒè€ƒä»¥ä¸‹ä»£è¡¨ä¾‹å¥é¢¨æ ¼ï¼Œä½†ä¸è¦é‡è¤‡ï¼š
        {examples_text}

        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
        {{
            "sentence": "ä¸­æ–‡å¥å­",
            "hint": "é€™å€‹å¥å­çš„ç¿»è­¯é‡é»æˆ–æç¤ºï¼ˆä¾‹å¦‚ï¼šæ³¨æ„æ™‚æ…‹ã€æ³¨æ„ä»‹ä¿‚è©ç­‰ï¼‰",
            "difficulty_level": {level},
            "grammar_points": ["æ¶‰åŠçš„æ–‡æ³•é»1", "æ¶‰åŠçš„æ–‡æ³•é»2"]
        }}
        """
        import time as _t

        user_prompt = f"è«‹ç”Ÿæˆä¸€å€‹é©åˆç·´ç¿’çš„ä¸­æ–‡å¥å­\n[variant_nonce={int(_t.time() * 1000)} ]"

        if not self.generate_model:
            result = {}
        else:
            result = self._call_model(
                self.generate_model, system_prompt, user_prompt, use_cache=False, timeout=25
            )

        if not isinstance(result, dict):
            result = {}

        # ğŸ”¥ é€æ˜åŒ–æ”¹é€ ï¼šä¸å†éš±è—æœå‹™éŒ¯èª¤ï¼
        if result.get("service_error"):
            # AI æœå‹™å¤±æ•—ï¼Œç›´æ¥è¿”å›éŒ¯èª¤ç‹€æ…‹ï¼Œä¸è¦æ¬ºé¨™ä¸Šå±¤ï¼
            return {
                "service_error": True,
                "error_message": "AI æœå‹™ä¸å¯ç”¨",
                "sentence": None,
                "hint": None,
            }

        # åªæœ‰çœŸæ­£æˆåŠŸæ™‚æ‰è¿”å›å…§å®¹
        import random as _r
        fallback_sentence = _r.choice(examples) if examples else "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚"
        return {
            "sentence": result.get("sentence", fallback_sentence),
            "hint": result.get("hint", "æ³¨æ„æ™‚æ…‹å’Œä¸»è©"),
            "difficulty_level": result.get("difficulty_level", level),
            "grammar_points": result.get("grammar_points", []),
            "service_error": False,  # æ˜ç¢ºæ¨™è¨˜é€™æ˜¯æˆåŠŸç‹€æ…‹
        }

    def generate_review_sentence(
        self,
        knowledge_points: list,
        level: int = 2,
        length: str = "medium",
    ) -> dict[str, Any]:
        """
        æ ¹æ“šå¾…è¤‡ç¿’çš„çŸ¥è­˜é»ç”Ÿæˆè¤‡ç¿’å¥å­ã€‚

        Args:
            knowledge_points: å¾…è¤‡ç¿’çš„çŸ¥è­˜é»åˆ—è¡¨ã€‚
            level: é›£åº¦ç­‰ç´šã€‚
            length: å¥å­é•·åº¦ã€‚

        Returns:
            åŒ…å«è¤‡ç¿’å¥å­ã€æç¤ºã€ç›®æ¨™çŸ¥è­˜é»IDç­‰è³‡è¨Šçš„å­—å…¸ã€‚
        """
        import random

        if not knowledge_points:
            return self.generate_practice_sentence(level=level, length=length)

        num_points = 2 if length == "long" else 1
        length_hint = "è¼ƒé•·å¥å­ï¼ˆ35-60å­—ï¼‰" if length == "long" else "ä¸­ç­‰é•·åº¦ï¼ˆ20-35å­—ï¼‰"

        candidates = knowledge_points[:5]
        selected_points = random.sample(candidates, min(len(candidates), num_points))

        points_info = [
            {
                "id": getattr(p, "id", p.get("id")),
                "key_point": getattr(p, "key_point", p.get("key_point")),
                "explanation": getattr(p, "explanation", p.get("explanation")),
                "original_phrase": getattr(p, "original_phrase", p.get("original_phrase")),
                "correction": getattr(p, "correction", p.get("correction")),
            }
            for p in selected_points
        ]

        self.logger.info(f"ç‚ºè¤‡ç¿’ç’°ç¯€é¸å–äº† {len(selected_points)} å€‹çŸ¥è­˜é»ã€‚")

        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡æ•™å¸«ï¼Œæ­£åœ¨å¹«åŠ©å­¸ç”Ÿè¤‡ç¿’ã€‚
        è«‹ä½¿ç”¨ä»¥ä¸‹çŸ¥è­˜é»è¨­è¨ˆä¸€å€‹{length_hint}çš„ä¸­æ–‡å¥å­è®“å­¸ç”Ÿç¿»è­¯ï¼š
        {json.dumps(points_info, ensure_ascii=False, indent=2)}

        è¦æ±‚ï¼š
        - å¿…é ˆè€ƒå¯Ÿåˆ°æ‰€æœ‰æä¾›çš„çŸ¥è­˜é»ã€‚
        - å¥å­è¦è‡ªç„¶ã€å¯¦ç”¨ï¼Œä¸è¦ç”Ÿç¡¬å †ç Œã€‚
        - ç¢ºä¿éŒ¯èª¤é»èƒ½åœ¨ç¿»è­¯ä¸­è¢«è‡ªç„¶è€ƒå¯Ÿåˆ°ã€‚

        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
        {{
            "sentence": "è¦ç¿»è­¯çš„ä¸­æ–‡å¥å­",
            "hint": "çµ¦å­¸ç”Ÿçš„æç¤ºï¼ˆç°¡æ½”æŒ‡å‡ºè¦æ³¨æ„ä»€éº¼ï¼‰",
            "target_point_ids": {[p["id"] for p in points_info]},
            "target_points_description": "æœ¬é¡Œè€ƒå¯Ÿï¼š{", ".join([p["key_point"] for p in points_info])}",
            "difficulty_level": {level}
        }}
        """
        user_prompt = "è«‹è¨­è¨ˆä¸€å€‹è¤‡ç¿’å¥å­ï¼Œè¦æœ‰å‰µæ„ä¸”æ¯æ¬¡éƒ½ä¸åŒã€‚"

        if not self.generate_model:
            return {
                "sentence": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚",
                "hint": "æ³¨æ„æ™‚æ…‹",
                "target_point_ids": [],
                "difficulty_level": level,
            }

        result = self._call_model(self.generate_model, system_prompt, user_prompt, use_cache=False, timeout=30)
        
        # ğŸ”¥ é€æ˜åŒ–æ”¹é€ ï¼šReview æ¨¡å¼çš„ AI æœå‹™å±¤ä¹Ÿè¦æª¢æŸ¥éŒ¯èª¤ï¼
        if result.get("service_error"):
            return {
                "service_error": True,
                "error_message": "AI æœå‹™ä¸å¯ç”¨",
                "sentence": None,
                "hint": None,
                "target_point_ids": [],
                "target_points": [],
                "target_points_description": "",
            }
        
        if not isinstance(result, dict):
            result = {}

        target_points = [
            {
                "id": getattr(p, "id", p.get("id")),
                "key_point": getattr(p, "key_point", p.get("key_point")),
                "category": getattr(p, "category", {}).get("value", "other"),
                "mastery_level": round(getattr(p, "mastery_level", 0.0), 2),
            }
            for p in selected_points
        ]

        return {
            "sentence": result.get("sentence", "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚"),
            "hint": result.get("hint", "æ³¨æ„ä¹‹å‰çš„éŒ¯èª¤"),
            "target_point_ids": [p["id"] for p in points_info],
            "target_points": target_points,
            "target_points_description": result.get("target_points_description", ""),
            "difficulty_level": result.get("difficulty_level", level),
            "is_review": True,
            "service_error": False,  # æ˜ç¢ºæ¨™è¨˜æˆåŠŸç‹€æ…‹
        }

    def generate_tagged_sentence(
        self,
        tags: list[dict[str, str]],
        level: int = 2,
        length: str = "medium",
        combination_mode: str = "all",
    ) -> dict[str, Any]:
        """
        åŸºæ–¼æ¨™ç±¤ç”Ÿæˆç·´ç¿’é¡Œç›®ã€‚

        Args:
            tags: æ¨™ç±¤åˆ—è¡¨ï¼Œä¾‹å¦‚ [{"type": "grammar", "id": "GP001", "name": "å¼·èª¿å¥"}]ã€‚
            level: é›£åº¦ç­‰ç´šã€‚
            length: å¥å­é•·åº¦ã€‚
            combination_mode: æ¨™ç±¤çµ„åˆæ¨¡å¼ ("all", "any", "focus")ã€‚

        Returns:
            åŒ…å«é¡Œç›®ã€æç¤ºã€è¦†è“‹é»ç­‰è³‡è¨Šçš„å­—å…¸ã€‚
        """
        import json

        from core.tag_system import tag_manager

        if not tags:
            return self.generate_practice_sentence(level=level, length=length)

        tag_details = []
        for tag_info in tags:
            tag_id = tag_info.get("id")
            if (
                tag_id
                and tag_manager.get_tag(tag_id)
                and (pattern := tag_manager.patterns_data.get(tag_id))
            ):
                tag_details.append(
                    {
                        "id": tag_id,
                        "pattern": pattern.get("pattern", ""),
                        "formula": pattern.get("formula", ""),
                        "core_concept": pattern.get("core_concept", ""),
                        "examples": pattern.get("examples", [])[:2],
                    }
                )

        if not tag_details:
            return self.generate_practice_sentence(level=level, length=length)

        length_hint = {
            "short": "ç°¡çŸ­å¥å­ï¼ˆ10-20å­—ï¼‰",
            "medium": "ä¸­ç­‰é•·åº¦ï¼ˆ20-35å­—ï¼‰",
            "long": "è¼ƒé•·å¥å­ï¼ˆ35-60å­—ï¼‰",
        }.get(length, "ä¸­ç­‰é•·åº¦")
        mode_instruction = {
            "all": "å¿…é ˆåŒæ™‚åŒ…å«ä¸¦æ­£ç¢ºå±•ç¤ºæ‰€æœ‰åˆ—å‡ºçš„æ–‡æ³•å¥å‹",
            "any": "é¸æ“‡å…¶ä¸­ä¸€å€‹æˆ–å¤šå€‹æ–‡æ³•å¥å‹ä¾†æ§‹å»ºå¥å­",
            "focus": "ä»¥ç¬¬ä¸€å€‹æ–‡æ³•å¥å‹ç‚ºä¸»ï¼Œå…¶ä»–å¯é¸æ“‡æ€§åŠ å…¥",
        }.get(combination_mode, "åŒ…å«æ‰€æœ‰æ–‡æ³•å¥å‹")

        system_prompt = f"""
        ä½ æ˜¯å°ˆæ¥­çš„è‹±æ–‡æ•™å¸«ï¼Œè«‹è¨­è¨ˆä¸€å€‹ç¿»è­¯ç·´ç¿’é¡Œç›®ã€‚
        è¦æ±‚ä½¿ç”¨çš„æ–‡æ³•å¥å‹ï¼š
        {json.dumps(tag_details, ensure_ascii=False, indent=2)}
        çµ„åˆè¦æ±‚ï¼š{mode_instruction}
        é¡Œç›®è¦æ±‚ï¼š
        1. è¨­è¨ˆä¸€å€‹{length_hint}çš„ä¸­æ–‡å¥å­ã€‚
        2. é›£åº¦ç­‰ç´šï¼š{level}/5ã€‚
        3. å¿…é ˆè‡ªç„¶åœ°èå…¥æŒ‡å®šçš„æ–‡æ³•å¥å‹ã€‚
        4. å¥å­è¦å¯¦ç”¨ã€è²¼è¿‘ç”Ÿæ´»æˆ–å·¥ä½œå ´æ™¯ã€‚
        5. é¿å…ç”Ÿç¡¬å †ç Œï¼Œç¢ºä¿èªæ„æµæš¢ã€‚
        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
        {{
            "sentence": "è¦ç¿»è­¯çš„ä¸­æ–‡å¥å­",
            "hint": "çµ¦å­¸ç”Ÿçš„æç¤ºï¼ˆç°¡æ½”æŒ‡å‡ºè¦æ³¨æ„çš„æ–‡æ³•é‡é»ï¼‰",
            "covered_points": ["å¯¦éš›ä½¿ç”¨çš„æ–‡æ³•é»1", "å¯¦éš›ä½¿ç”¨çš„æ–‡æ³•é»2"],
            "expected_patterns": ["é æœŸä½¿ç”¨çš„å¥å‹çµæ§‹"],
            "difficulty_level": {level}
        }}
        """
        user_prompt = "è«‹è¨­è¨ˆä¸€å€‹åŒ…å«æŒ‡å®šæ–‡æ³•å¥å‹çš„ç·´ç¿’é¡Œç›®ã€‚"

        if not self.generate_model:
            return {
                "sentence": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚",
                "hint": "æ³¨æ„å¥å‹çµæ§‹",
                "covered_points": [],
                "difficulty_level": level,
            }

        result = self._call_model(self.generate_model, system_prompt, user_prompt, use_cache=False, timeout=35)
        if not isinstance(result, dict):
            result = {}

        return {
            "sentence": result.get("sentence", "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚"),
            "hint": result.get("hint", "æ³¨æ„æ–‡æ³•å¥å‹çš„æ­£ç¢ºä½¿ç”¨"),
            "covered_points": result.get("covered_points", []),
            "expected_patterns": result.get("expected_patterns", []),
            "tags": tags,
            "combination_mode": combination_mode,
            "difficulty_level": result.get("difficulty_level", level),
            "is_tagged": True,
        }

    def generate_tagged_preview(
        self,
        tags: list[str],
        combination_mode: str = "all",
    ) -> dict[str, Any]:
        """
        ç”Ÿæˆæ¨™ç±¤çµ„åˆçš„é è¦½é¡Œç›®ï¼Œä¸¦è©•ä¼°å…¶é›£åº¦å’Œè®ŠåŒ–æ€§ã€‚

        Args:
            tags: æ¨™ç±¤IDåˆ—è¡¨ã€‚
            combination_mode: çµ„åˆæ¨¡å¼ã€‚

        Returns:
            åŒ…å«é è¦½é¡Œç›®å’Œè©•ä¼°åˆ†æ•¸çš„å­—å…¸ã€‚
        """
        tag_list = [{"type": "grammar", "id": tag_id} for tag_id in tags]
        result = self.generate_tagged_sentence(
            tags=tag_list, level=2, length="medium", combination_mode=combination_mode
        )

        from core.tag_system import tag_manager

        complexity_scores = [
            tag.complexity for tag_id in tags if (tag := tag_manager.get_tag(tag_id))
        ]
        avg_complexity = sum(complexity_scores) / len(complexity_scores) if complexity_scores else 2
        variety_score = min(100, len(tags) * 30)

        if not isinstance(result, dict):
            result = {"sentence": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚", "hint": "æ³¨æ„å¥å‹çµæ§‹"}

        result["difficulty"] = min(5, max(1, round(avg_complexity)))
        result["variety_score"] = variety_score
        return result

    def get_last_interaction(self) -> dict[str, Any]:
        """
        ç²å–æœ€è¿‘ä¸€æ¬¡èˆ‡ LLM çš„äº’å‹•è¨˜éŒ„ï¼Œç”¨æ–¼èª¿è©¦ã€‚

        Returns:
            ä¸€å€‹åŒ…å«äº’å‹•è©³æƒ…çš„å­—å…¸ï¼Œå¦‚æœæ²’æœ‰è¨˜éŒ„å‰‡è¿”å›æç¤ºè¨Šæ¯ã€‚
        """
        return self.last_llm_interaction or {"status": "no_data", "message": "å°šç„¡ LLM äº’å‹•è¨˜éŒ„"}

    def analyze_common_mistakes(self, practice_history: list) -> dict[str, Any]:
        """
        åˆ†ææœ€è¿‘çš„ç·´ç¿’æ­·å²ï¼Œæ‰¾å‡ºå¸¸è¦‹çš„éŒ¯èª¤æ¨¡å¼ä¸¦æä¾›å­¸ç¿’å»ºè­°ã€‚

        Args:
            practice_history: ç·´ç¿’æ­·å²è¨˜éŒ„åˆ—è¡¨ã€‚

        Returns:
            åŒ…å«éŒ¯èª¤æ¨¡å¼åˆ†æå’Œå­¸ç¿’å»ºè­°çš„å­—å…¸ã€‚
        """
        if not practice_history:
            return {"patterns": [], "suggestions": []}

        recent_mistakes = [
            {
                "nature": error.get("error_nature", ""),
                "key_point": error.get("key_point_summary", ""),
            }
            for practice in practice_history[-10:]
            if not practice.get("is_correct", True)
            for error in practice.get("feedback", {}).get("error_analysis", [])
        ]

        if not recent_mistakes:
            return {"patterns": [], "suggestions": []}

        system_prompt = """
        ä½ æ˜¯ä¸€ä½è‹±èªæ•™å­¸å°ˆå®¶ï¼Œè«‹åˆ†æå­¸ç”Ÿçš„éŒ¯èª¤æ¨¡å¼ä¸¦æä¾›å­¸ç¿’å»ºè­°ã€‚
        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
        {
            "common_patterns": [{"pattern": "éŒ¯èª¤æ¨¡å¼æè¿°", "frequency": "å‡ºç¾é »ç‡ï¼ˆé«˜/ä¸­/ä½ï¼‰", "examples": ["ä¾‹å­1", "ä¾‹å­2"]}],
            "learning_suggestions": [{"priority": "å„ªå…ˆç´šï¼ˆé«˜/ä¸­/ä½ï¼‰", "focus_area": "éœ€è¦åŠ å¼·çš„é ˜åŸŸ", "specific_advice": "å…·é«”çš„å­¸ç¿’å»ºè­°", "resources": ["å»ºè­°çš„å­¸ç¿’è³‡æºæˆ–æ–¹æ³•"]}],
            "overall_assessment": "æ•´é«”è©•ä¼°å’Œé¼“å‹µçš„è©±"
        }
        """
        user_prompt = f"ä»¥ä¸‹æ˜¯å­¸ç”Ÿæœ€è¿‘çš„éŒ¯èª¤è¨˜éŒ„ï¼š\n{json.dumps(recent_mistakes, ensure_ascii=False, indent=2)}"

        if not self.grade_model:
            return {"patterns": [], "suggestions": []}

        result = self._call_model(self.grade_model, system_prompt, user_prompt, timeout=25)
        if not isinstance(result, dict):
            return {"patterns": [], "suggestions": []}
        return result

    def generate_sentence_for_pattern(
        self,
        pattern_data: dict,
        level: int = 2,
        length: str = "medium",
    ) -> dict[str, Any]:
        """
        æ ¹æ“šç‰¹å®šçš„æ–‡æ³•å¥å‹ç”Ÿæˆç·´ç¿’é¡Œç›®ã€‚

        Args:
            pattern_data: åŒ…å«å¥å‹è©³æƒ…çš„å­—å…¸ã€‚
            level: é›£åº¦ç­‰ç´šã€‚
            length: å¥å­é•·åº¦ã€‚

        Returns:
            åŒ…å«é¡Œç›®ã€æç¤ºå’Œé æœŸçµæ§‹çš„å­—å…¸ã€‚
        """
        pattern_name = pattern_data.get("pattern", "")
        formula = pattern_data.get("formula", "")
        core_concept = pattern_data.get("core_concept", "")
        examples = pattern_data.get("examples", [])
        example_text = "\n".join(
            f"   ä¾‹{i + 1}. {ex.get('zh', '')} â†’ {ex.get('en', '')}"
            for i, ex in enumerate(examples[:3])
            if isinstance(ex, dict) and ex.get("zh") and ex.get("en")
        )

        length_hint = {
            "short": "ç°¡çŸ­å¥å­ï¼ˆ10-20å­—ï¼‰",
            "medium": "ä¸­ç­‰é•·åº¦ï¼ˆ20-35å­—ï¼‰",
            "long": "è¼ƒé•·å¥å­ï¼ˆ35-60å­—ï¼‰",
        }.get(length, "ä¸­ç­‰é•·åº¦")
        difficulty_hint = {
            1: "åŸºç¤ç¨‹åº¦",
            2: "ä¸­ç´šç¨‹åº¦",
            3: "ä¸­é«˜ç´šç¨‹åº¦",
            4: "é«˜ç´šç¨‹åº¦",
            5: "é€²éšç¨‹åº¦",
        }.get(level, "ä¸­ç´šç¨‹åº¦")

        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡æ•™å¸«ï¼Œè«‹æ ¹æ“šæŒ‡å®šçš„æ–‡æ³•å¥å‹ç”Ÿæˆä¸€å€‹ä¸­æ–‡ç·´ç¿’å¥å­ã€‚
        ç›®æ¨™å¥å‹ï¼š{pattern_name}
        å¥å‹å…¬å¼ï¼š{formula}
        æ ¸å¿ƒæ¦‚å¿µï¼š{core_concept}
        åƒè€ƒä¾‹å¥ï¼š{example_text or "ç„¡"}
        è¦æ±‚ï¼š
        1. ç”Ÿæˆä¸€å€‹{length_hint}çš„ä¸­æ–‡å¥å­ã€‚
        2. é›£åº¦ç‚º{difficulty_hint}ã€‚
        3. å¥å­å¿…é ˆèƒ½å¤ è‡ªç„¶åœ°ä½¿ç”¨ã€Œ{pattern_name}ã€å¥å‹ä¾†ç¿»è­¯ã€‚
        4. å¥å­è¦å¯¦ç”¨ã€è²¼è¿‘ç”Ÿæ´»æˆ–å·¥ä½œå ´æ™¯ã€‚
        5. ä¸è¦ç›´æ¥è¤‡è£½ä¾‹å¥ï¼Œè¦æœ‰å‰µæ„ã€‚
        è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
        {{
            "sentence": "è¦ç¿»è­¯çš„ä¸­æ–‡å¥å­",
            "hint": "çµ¦å­¸ç”Ÿçš„æç¤ºï¼ˆç°¡çŸ­èªªæ˜é€™å€‹å¥å‹çš„ä½¿ç”¨è¦é»ï¼‰",
            "expected_structure": "é æœŸçš„è‹±æ–‡å¥å‹çµæ§‹ï¼ˆä½¿ç”¨ ... è¡¨ç¤ºå¯å¡«å…¥çš„éƒ¨åˆ†ï¼‰"
        }}
        """
        user_prompt = "è«‹ç”Ÿæˆä¸€å€‹é©åˆç·´ç¿’æ­¤æ–‡æ³•å¥å‹çš„ä¸­æ–‡å¥å­ã€‚"

        if not self.generate_model:
            return {
                "sentence": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚",
                "hint": f"æ³¨æ„ä½¿ç”¨ {pattern_name} å¥å‹",
                "expected_structure": "",
            }

        result = self._call_model(self.generate_model, system_prompt, user_prompt, use_cache=False, timeout=30)
        
        # ğŸ”¥ é€æ˜åŒ–æ”¹é€ ï¼šPattern æ¨¡å¼ä¹Ÿè¦æª¢æŸ¥æœå‹™éŒ¯èª¤ï¼
        if result.get("service_error"):
            return {
                "service_error": True,
                "error_message": "AI æœå‹™ä¸å¯ç”¨",
                "sentence": None,
                "hint": None,
                "expected_structure": None,
            }
        
        if not isinstance(result, dict):
            result = {"sentence": "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚", "hint": f"æ³¨æ„ä½¿ç”¨ {pattern_name} å¥å‹"}

        return {
            "sentence": result.get("sentence", "ä»Šå¤©å¤©æ°£å¾ˆå¥½ã€‚"),
            "hint": result.get("hint", f"æ³¨æ„ä½¿ç”¨ {pattern_name} å¥å‹"),
            "expected_structure": result.get("expected_structure", formula),
            "service_error": False,  # æ˜ç¢ºæ¨™è¨˜æˆåŠŸç‹€æ…‹
        }
