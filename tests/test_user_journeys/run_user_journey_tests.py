"""
ç”¨æˆ¶æ“ä½œè·¯å¾‘ä¸€è‡´æ€§æ¸¬è©¦åŸ·è¡Œå™¨
æ•´åˆæ‰€æœ‰ç”¨æˆ¶è·¯å¾‘æ¸¬è©¦ï¼Œç”Ÿæˆä¸€è‡´æ€§å ±å‘Š
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Dict, List

import pytest

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


class UserJourneyTestRunner:
    """ç”¨æˆ¶è·¯å¾‘æ¸¬è©¦åŸ·è¡Œå™¨"""

    def __init__(self):
        self.test_results: List[Dict] = []
        self.start_time = None
        self.end_time = None

    async def run_all_tests(self) -> Dict:
        """åŸ·è¡Œæ‰€æœ‰ç”¨æˆ¶è·¯å¾‘æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹åŸ·è¡Œç”¨æˆ¶æ“ä½œè·¯å¾‘ä¸€è‡´æ€§é©—è­‰...")
        self.start_time = time.time()

        # å®šç¾©æ¸¬è©¦æ¨¡çµ„
        test_modules = [
            "test_new_user_experience",
            "test_daily_practice_flow",
            "test_knowledge_management",
            "test_search_and_statistics"
        ]

        overall_results = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'test_details': [],
            'execution_time': 0,
            'consistency_score': 0.0
        }

        for module in test_modules:
            print(f"\nğŸ“‹ åŸ·è¡Œæ¸¬è©¦æ¨¡çµ„: {module}")
            module_result = await self._run_test_module(module)
            overall_results['test_details'].append(module_result)
            overall_results['total_tests'] += module_result['total']
            overall_results['passed_tests'] += module_result['passed']
            overall_results['failed_tests'] += module_result['failed']

        self.end_time = time.time()
        overall_results['execution_time'] = self.end_time - self.start_time

        # è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸
        if overall_results['total_tests'] > 0:
            overall_results['consistency_score'] = overall_results['passed_tests'] / overall_results['total_tests']

        self._generate_report(overall_results)

        return overall_results

    async def _run_test_module(self, module_name: str) -> Dict:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦æ¨¡çµ„"""
        module_result = {
            'module': module_name,
            'total': 0,
            'passed': 0,
            'failed': 0,
            'errors': [],
            'execution_time': 0
        }

        start_time = time.time()

        try:
            # ä½¿ç”¨ pytest åŸ·è¡Œæ¸¬è©¦
            test_file = f"tests/test_user_journeys/{module_name}.py"

            # åŸ·è¡Œ pytest ä¸¦æ•ç²çµæœ
            result = pytest.main([
                test_file,
                "-v",
                "--tb=short",
                "-x"  # é‡åˆ°ç¬¬ä¸€å€‹å¤±æ•—å°±åœæ­¢
            ])

            # ç°¡åŒ–çµæœè™•ç†
            if result == 0:
                module_result['passed'] = 1
                module_result['total'] = 1
                print(f"âœ… {module_name}: é€šé")
            else:
                module_result['failed'] = 1
                module_result['total'] = 1
                module_result['errors'].append(f"æ¨¡çµ„ {module_name} åŸ·è¡Œå¤±æ•—")
                print(f"âŒ {module_name}: å¤±æ•—")

        except Exception as e:
            module_result['failed'] = 1
            module_result['total'] = 1
            module_result['errors'].append(str(e))
            print(f"âŒ {module_name}: ç•°å¸¸ - {e}")

        module_result['execution_time'] = time.time() - start_time
        return module_result

    def _generate_report(self, results: Dict) -> None:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print(f"\n{'='*60}")
        print("ğŸ“Š ç”¨æˆ¶æ“ä½œè·¯å¾‘ä¸€è‡´æ€§é©—è­‰å ±å‘Š")
        print(f"{'='*60}")

        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {results['execution_time']:.2f} ç§’")
        print(f"ğŸ“ˆ æ¸¬è©¦ç¸½æ•¸: {results['total_tests']}")
        print(f"âœ… é€šéæ¸¬è©¦: {results['passed_tests']}")
        print(f"âŒ å¤±æ•—æ¸¬è©¦: {results['failed_tests']}")
        print(f"ğŸ¯ ä¸€è‡´æ€§åˆ†æ•¸: {results['consistency_score']:.1%}")

        # ä¸€è‡´æ€§è©•ç´š
        if results['consistency_score'] >= 0.95:
            consistency_grade = "ğŸ† å„ªç§€"
        elif results['consistency_score'] >= 0.85:
            consistency_grade = "ğŸ¥ˆ è‰¯å¥½"
        elif results['consistency_score'] >= 0.70:
            consistency_grade = "ğŸ¥‰ åŠæ ¼"
        else:
            consistency_grade = "âš ï¸ éœ€æ”¹é€²"

        print(f"ğŸ“‹ ä¸€è‡´æ€§è©•ç´š: {consistency_grade}")

        print(f"\n{'='*30} è©³ç´°çµæœ {'='*30}")

        for detail in results['test_details']:
            status = "âœ…" if detail['failed'] == 0 else "âŒ"
            print(f"{status} {detail['module']}: {detail['passed']}/{detail['total']} é€šé ({detail['execution_time']:.2f}s)")

            if detail['errors']:
                for error in detail['errors']:
                    print(f"   âš ï¸ {error}")

        print(f"\n{'='*60}")

        # çµ¦å‡ºå»ºè­°
        if results['consistency_score'] < 0.85:
            print("ğŸ”§ å»ºè­°:")
            print("   - æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦æ¡ˆä¾‹")
            print("   - åˆ†æå…©ç¨®æ¨¡å¼é–“çš„å·®ç•°åŸå› ")
            print("   - å„ªåŒ–çµ±è¨ˆè¨ˆç®—é‚è¼¯çš„ä¸€è‡´æ€§")
            print("   - æª¢æŸ¥æ•¸æ“šåŒæ­¥æ©Ÿåˆ¶")
        else:
            print("ğŸ‰ ç³»çµ±ä¸€è‡´æ€§è¡¨ç¾å„ªç§€ï¼")
            print("   - ç”¨æˆ¶é«”é©—åœ¨å…©ç¨®æ¨¡å¼ä¸‹é«˜åº¦ä¸€è‡´")
            print("   - å¯ä»¥å®‰å…¨åœ°åœ¨å…©ç¨®æ¨¡å¼é–“åˆ‡æ›")


async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    runner = UserJourneyTestRunner()

    try:
        results = await runner.run_all_tests()

        # è¿”å›é©ç•¶çš„é€€å‡ºä»£ç¢¼
        if results['consistency_score'] >= 0.7:
            print("\nğŸ‰ ç”¨æˆ¶æ“ä½œè·¯å¾‘ä¸€è‡´æ€§é©—è­‰å®Œæˆï¼")
            return 0
        else:
            print("\nâš ï¸ ä¸€è‡´æ€§é©—è­‰ç™¼ç¾å•é¡Œï¼Œéœ€è¦é€²ä¸€æ­¥æ”¹é€²")
            return 1

    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå™¨ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 2


if __name__ == "__main__":
    exit_code = asyncio.run(main())
