#!/usr/bin/env python3
"""
çŸ¥è­˜é»æ•¸æ“šé·ç§»è…³æœ¬
è§£æ±ºçŸ¥è­˜é»é‡è¤‡å•é¡Œï¼Œå°‡æ··åˆçš„çŸ¥è­˜é»æ‹†åˆ†ç‚ºç¨ç«‹çš„çŸ¥è­˜é»
"""

import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.error_types import ErrorCategory


def load_knowledge_data(file_path: Path) -> Dict[str, Any]:
    """è¼‰å…¥çŸ¥è­˜é»è³‡æ–™"""
    if not file_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ: {file_path}")
        return {"version": "2.0", "data": []}
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… è¼‰å…¥äº† {len(data.get('data', []))} å€‹çŸ¥è­˜é»")
    return data


def create_backup(file_path: Path):
    """å‰µå»ºå‚™ä»½æª”æ¡ˆ"""
    if not file_path.exists():
        return
    
    backup_dir = file_path.parent / "backups"
    backup_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = backup_dir / f"{file_path.stem}_migration_backup_{timestamp}.json"
    
    import shutil
    shutil.copy2(file_path, backup_path)
    print(f"âœ… å‚™ä»½å·²å‰µå»º: {backup_path}")


def analyze_duplicates(knowledge_points: List[Dict]) -> Dict[str, List[Dict]]:
    """åˆ†æé‡è¤‡çš„çŸ¥è­˜é»"""
    groups = defaultdict(list)
    
    for point in knowledge_points:
        key_point = point.get("key_point", "")
        groups[key_point].append(point)
    
    # åªè¿”å›æœ‰é‡è¤‡çš„ç¾¤çµ„
    duplicates = {key: points for key, points in groups.items() if len(points) > 1}
    
    print(f"ğŸ” ç™¼ç¾ {len(duplicates)} å€‹é‡è¤‡çš„çŸ¥è­˜é»åˆ†é¡:")
    for key, points in duplicates.items():
        print(f"  - '{key}': {len(points)} å€‹é‡è¤‡")
        for point in points:
            original = point.get("original_phrase", "")
            correction = point.get("correction", "")
            examples_count = len(point.get("examples", []))
            print(f"    * ID:{point['id']} {original}â†’{correction} ({examples_count}ä¾‹å¥)")
    
    return duplicates


def migrate_knowledge_point(old_point: Dict) -> Dict:
    """é·ç§»å–®å€‹çŸ¥è­˜é»åˆ°æ–°æ ¼å¼"""
    examples = old_point.get("examples", [])
    
    # ç¬¬ä¸€å€‹ä¾‹å¥ä½œç‚ºåŸå§‹éŒ¯èª¤
    if examples:
        first_example = examples[0]
        original_error = {
            "chinese_sentence": first_example.get("chinese", ""),
            "user_answer": first_example.get("user_answer", ""),
            "correct_answer": first_example.get("correct", ""),
            "timestamp": old_point.get("created_at", datetime.now().isoformat())
        }
        
        # å…¶é¤˜ä¾‹å¥ä½œç‚ºè¤‡ç¿’è¨˜éŒ„
        review_examples = []
        for example in examples[1:]:
            review_examples.append({
                "chinese_sentence": example.get("chinese", ""),
                "user_answer": example.get("user_answer", ""),
                "correct_answer": example.get("correct", ""),
                "timestamp": old_point.get("last_seen", datetime.now().isoformat()),
                "is_correct": False  # å‡è¨­éƒ½æ˜¯éŒ¯èª¤è¨˜éŒ„
            })
    else:
        # å¦‚æœæ²’æœ‰ä¾‹å¥ï¼Œå‰µå»ºç©ºçš„åŸå§‹éŒ¯èª¤
        original_error = {
            "chinese_sentence": "",
            "user_answer": old_point.get("original_phrase", ""),
            "correct_answer": old_point.get("correction", ""),
            "timestamp": old_point.get("created_at", datetime.now().isoformat())
        }
        review_examples = []
    
    # ç”Ÿæˆæ›´å…·é«”çš„ key_point
    original_phrase = old_point.get("original_phrase", "")
    if original_phrase and original_phrase not in old_point.get("key_point", ""):
        specific_key_point = f"{old_point['key_point']}: {original_phrase}"
    else:
        specific_key_point = old_point["key_point"]
    
    # å‰µå»ºæ–°æ ¼å¼çš„çŸ¥è­˜é»
    new_point = {
        "id": old_point["id"],
        "key_point": specific_key_point,
        "category": old_point.get("category", "isolated"),
        "subtype": old_point.get("subtype", "vocabulary"),
        "explanation": old_point.get("explanation", ""),
        "original_phrase": old_point.get("original_phrase", ""),
        "correction": old_point.get("correction", ""),
        "original_error": original_error,
        "review_examples": review_examples,
        "mastery_level": old_point.get("mastery_level", 0.0),
        "mistake_count": old_point.get("mistake_count", 1),
        "correct_count": old_point.get("correct_count", 0),
        "created_at": old_point.get("created_at", datetime.now().isoformat()),
        "last_seen": old_point.get("last_seen", datetime.now().isoformat()),
        "next_review": old_point.get("next_review", datetime.now().isoformat())
    }
    
    return new_point


def split_duplicate_knowledge_points(duplicates: Dict[str, List[Dict]], next_id: int) -> List[Dict]:
    """æ‹†åˆ†é‡è¤‡çš„çŸ¥è­˜é»"""
    new_points = []
    
    print("ğŸ”§ é–‹å§‹æ‹†åˆ†é‡è¤‡çš„çŸ¥è­˜é»...")
    
    for key_point, points in duplicates.items():
        print(f"\nè™•ç†é‡è¤‡åˆ†é¡: '{key_point}'")
        
        for i, point in enumerate(points):
            original_phrase = point.get("original_phrase", "")
            correction = point.get("correction", "")
            
            # ç‚ºæ¯å€‹ä¸åŒçš„éŒ¯èª¤å‰µå»ºç¨ç«‹çŸ¥è­˜é»
            if i == 0:
                # ç¬¬ä¸€å€‹ä¿æŒåŸID
                new_point = migrate_knowledge_point(point)
            else:
                # å…¶ä»–çš„åˆ†é…æ–°ID
                new_point = migrate_knowledge_point(point)
                new_point["id"] = next_id
                next_id += 1
                print(f"  âœ¨ å‰µå»ºæ–°çŸ¥è­˜é» ID:{new_point['id']} - {original_phrase}â†’{correction}")
            
            new_points.append(new_point)
    
    return new_points, next_id


def migrate_all_knowledge_points(knowledge_points: List[Dict]) -> List[Dict]:
    """é·ç§»æ‰€æœ‰çŸ¥è­˜é»"""
    print("\nğŸ”§ é–‹å§‹é·ç§»æ‰€æœ‰çŸ¥è­˜é»åˆ°æ–°æ ¼å¼...")
    
    # åˆ†æé‡è¤‡æƒ…æ³
    duplicates = analyze_duplicates(knowledge_points)
    
    # ç²å–ä¸‹ä¸€å€‹å¯ç”¨çš„ID
    max_id = max(point["id"] for point in knowledge_points) if knowledge_points else 0
    next_id = max_id + 1
    
    # æ”¶é›†æ‰€æœ‰é·ç§»å¾Œçš„çŸ¥è­˜é»
    migrated_points = []
    processed_ids = set()
    
    # è™•ç†é‡è¤‡çš„çŸ¥è­˜é»
    if duplicates:
        duplicate_points, next_id = split_duplicate_knowledge_points(duplicates, next_id)
        migrated_points.extend(duplicate_points)
        
        # è¨˜éŒ„å·²è™•ç†çš„ID
        for key_point, points in duplicates.items():
            for point in points:
                processed_ids.add(point["id"])
    
    # è™•ç†éé‡è¤‡çš„çŸ¥è­˜é»
    for point in knowledge_points:
        if point["id"] not in processed_ids:
            migrated_point = migrate_knowledge_point(point)
            migrated_points.append(migrated_point)
    
    print(f"\nâœ… é·ç§»å®Œæˆ: {len(knowledge_points)} â†’ {len(migrated_points)} å€‹çŸ¥è­˜é»")
    
    return migrated_points


def save_migrated_data(migrated_points: List[Dict], output_path: Path):
    """å„²å­˜é·ç§»å¾Œçš„è³‡æ–™"""
    output_data = {
        "version": "3.0",  # å‡ç´šç‰ˆæœ¬è™Ÿ
        "migration_date": datetime.now().isoformat(),
        "data": migrated_points
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… é·ç§»è³‡æ–™å·²å„²å­˜: {output_path}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹çŸ¥è­˜é»æ•¸æ“šé·ç§»...")
    
    # è¨­å®šè·¯å¾‘
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"
    knowledge_file = data_dir / "knowledge.json"
    
    # å‰µå»ºå‚™ä»½
    create_backup(knowledge_file)
    
    # è¼‰å…¥ç¾æœ‰è³‡æ–™
    data = load_knowledge_data(knowledge_file)
    knowledge_points = data.get("data", [])
    
    if not knowledge_points:
        print("âš ï¸  æ²’æœ‰æ‰¾åˆ°éœ€è¦é·ç§»çš„è³‡æ–™")
        return
    
    # åŸ·è¡Œé·ç§»
    migrated_points = migrate_all_knowledge_points(knowledge_points)
    
    # å„²å­˜çµæœ
    save_migrated_data(migrated_points, knowledge_file)
    
    print("\nğŸ‰ çŸ¥è­˜é»æ•¸æ“šé·ç§»å®Œæˆï¼")
    print("\nğŸ“‹ é·ç§»æ‘˜è¦:")
    print(f"  - åŸå§‹çŸ¥è­˜é»æ•¸é‡: {len(knowledge_points)}")
    print(f"  - é·ç§»å¾ŒçŸ¥è­˜é»æ•¸é‡: {len(migrated_points)}")
    print(f"  - ç‰ˆæœ¬: 2.0 â†’ 3.0")
    print("\nğŸ“š ç¾åœ¨æ¯å€‹çŸ¥è­˜é»éƒ½å°æ‡‰ä¸€å€‹å…·é«”çš„éŒ¯èª¤æ¨¡å¼ï¼Œä¸å†æ··åˆä¸åŒçš„éŒ¯èª¤ï¼")


if __name__ == "__main__":
    main()