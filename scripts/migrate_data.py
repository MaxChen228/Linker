#!/usr/bin/env python3
"""
è³‡æ–™é·ç§»è…³æœ¬ï¼šå¾ JSON é·ç§»åˆ° PostgreSQL
æ”¯æ´å®Œæ•´çš„è³‡æ–™é·ç§»å’Œé©—è­‰åŠŸèƒ½
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.config import DATA_DIR
from core.database.connection import get_database_connection
from core.database.exceptions import DatabaseError
from core.database.repositories.knowledge_repository import KnowledgePointRepository
from core.error_types import ErrorCategory
from core.knowledge import KnowledgePoint, OriginalError, ReviewExample
from core.log_config import get_module_logger

logger = get_module_logger("migrate_data")


class DataMigrator:
    """è³‡æ–™é·ç§»å™¨"""

    def __init__(self):
        self.db_connection = get_database_connection()
        self.repository: Optional[KnowledgePointRepository] = None
        self.stats = {
            "total_points": 0,
            "migrated_points": 0,
            "errors": 0,
            "start_time": None,
            "end_time": None,
        }

    async def initialize(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£ç·šå’Œ Repository"""
        try:
            pool = await self.db_connection.connect()
            if not pool:
                raise DatabaseError("ç„¡æ³•å»ºç«‹è³‡æ–™åº«é€£ç·š")

            self.repository = KnowledgePointRepository(pool)
            logger.info("è³‡æ–™åº«é€£ç·šåˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±æ•—: {e}")
            return False

    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.db_connection:
            await self.db_connection.disconnect()

    def load_json_data(self, json_file: Path) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥ JSON è³‡æ–™"""
        try:
            if not json_file.exists():
                logger.error(f"JSON æª”æ¡ˆä¸å­˜åœ¨: {json_file}")
                return None

            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)

            logger.info(f"æˆåŠŸè¼‰å…¥ JSON æª”æ¡ˆ: {json_file}")
            logger.info(f"è³‡æ–™ç‰ˆæœ¬: {data.get('version', 'unknown')}")
            logger.info(f"çŸ¥è­˜é»æ•¸é‡: {len(data.get('data', []))}")

            return data
        except Exception as e:
            logger.error(f"è¼‰å…¥ JSON æª”æ¡ˆå¤±æ•—: {e}")
            return None

    def json_to_knowledge_point(self, point_data: Dict[str, Any]) -> Optional[KnowledgePoint]:
        """å°‡ JSON è³‡æ–™è½‰æ›ç‚º KnowledgePoint ç‰©ä»¶"""
        try:
            # è™•ç†åŸå§‹éŒ¯èª¤
            original_error_data = point_data.get("original_error", {})
            original_error = OriginalError(
                chinese_sentence=original_error_data.get("chinese_sentence", ""),
                user_answer=original_error_data.get("user_answer", ""),
                correct_answer=original_error_data.get("correct_answer", ""),
                timestamp=original_error_data.get("timestamp", datetime.now().isoformat()),
            )

            # è™•ç†è¤‡ç¿’ä¾‹å¥
            review_examples = []
            for example_data in point_data.get("review_examples", []):
                review_example = ReviewExample(
                    chinese_sentence=example_data.get("chinese_sentence", ""),
                    user_answer=example_data.get("user_answer", ""),
                    correct_answer=example_data.get("correct_answer", ""),
                    timestamp=example_data.get("timestamp", datetime.now().isoformat()),
                    is_correct=example_data.get("is_correct", False),
                )
                review_examples.append(review_example)

            # å»ºç«‹ KnowledgePoint
            knowledge_point = KnowledgePoint(
                id=point_data.get("id", 0),
                key_point=point_data.get("key_point", ""),
                category=ErrorCategory.from_string(point_data.get("category", "")),
                subtype=point_data.get("subtype", ""),
                explanation=point_data.get("explanation", ""),
                original_phrase=point_data.get("original_phrase", ""),
                correction=point_data.get("correction", ""),
                original_error=original_error,
                review_examples=review_examples,
                mastery_level=float(point_data.get("mastery_level", 0.0)),
                mistake_count=int(point_data.get("mistake_count", 1)),
                correct_count=int(point_data.get("correct_count", 0)),
                created_at=point_data.get("created_at", datetime.now().isoformat()),
                last_seen=point_data.get("last_seen", datetime.now().isoformat()),
                next_review=point_data.get("next_review", ""),
                is_deleted=point_data.get("is_deleted", False),
                deleted_at=point_data.get("deleted_at", ""),
                deleted_reason=point_data.get("deleted_reason", ""),
                tags=point_data.get("tags", []),
                custom_notes=point_data.get("custom_notes", ""),
                version_history=point_data.get("version_history", []),
                last_modified=point_data.get("last_modified", datetime.now().isoformat()),
            )

            return knowledge_point

        except Exception as e:
            logger.error(f"è½‰æ›çŸ¥è­˜é»è³‡æ–™å¤±æ•—: {e}, è³‡æ–™: {point_data.get('id', 'unknown')}")
            return None

    async def migrate_knowledge_points(
        self, json_data: Dict[str, Any], batch_size: int = 10
    ) -> bool:
        """é·ç§»çŸ¥è­˜é»è³‡æ–™"""
        if not self.repository:
            logger.error("Repository æœªåˆå§‹åŒ–")
            return False

        points_data = json_data.get("data", [])
        self.stats["total_points"] = len(points_data)
        self.stats["start_time"] = datetime.now()

        logger.info(f"é–‹å§‹é·ç§» {self.stats['total_points']} å€‹çŸ¥è­˜é»...")

        # æ‰¹æ¬¡è™•ç†
        for i in range(0, len(points_data), batch_size):
            batch = points_data[i : i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(points_data) + batch_size - 1) // batch_size

            logger.info(f"è™•ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡ ({len(batch)} å€‹çŸ¥è­˜é»)...")

            async with self.repository.transaction() as conn:
                for point_data in batch:
                    try:
                        # è½‰æ›è³‡æ–™
                        knowledge_point = self.json_to_knowledge_point(point_data)
                        if not knowledge_point:
                            self.stats["errors"] += 1
                            continue

                        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ (æ ¹æ“š key_point åˆ¤æ–·)
                        existing_points = await self.repository.search(knowledge_point.key_point)
                        if any(ep.key_point == knowledge_point.key_point for ep in existing_points):
                            logger.debug(f"çŸ¥è­˜é»å·²å­˜åœ¨ï¼Œè·³é: {knowledge_point.key_point}")
                            continue

                        # æ’å…¥è³‡æ–™åº«
                        await self.repository.create(knowledge_point)
                        self.stats["migrated_points"] += 1

                        # é€²åº¦å ±å‘Š
                        if self.stats["migrated_points"] % 50 == 0:
                            progress = (
                                self.stats["migrated_points"] / self.stats["total_points"]
                            ) * 100
                            logger.info(
                                f"å·²é·ç§»: {self.stats['migrated_points']}/{self.stats['total_points']} ({progress:.1f}%)"
                            )

                    except Exception as e:
                        logger.error(f"é·ç§»çŸ¥è­˜é»å¤±æ•—: {e}")
                        self.stats["errors"] += 1

        self.stats["end_time"] = datetime.now()
        return True

    async def verify_migration(self, json_data: Dict[str, Any]) -> bool:
        """é©—è­‰é·ç§»çµæœ"""
        logger.info("é–‹å§‹é©—è­‰é·ç§»çµæœ...")

        if not self.repository:
            logger.error("Repository æœªåˆå§‹åŒ–")
            return False

        try:
            # ç²å–è³‡æ–™åº«ä¸­çš„çµ±è¨ˆ
            db_stats = await self.repository.get_statistics()
            json_points = json_data.get("data", [])

            logger.info("é·ç§»é©—è­‰çµæœ:")
            logger.info(f"  JSON ä¸­çš„çŸ¥è­˜é»æ•¸é‡: {len(json_points)}")
            logger.info(f"  è³‡æ–™åº«ä¸­çš„çŸ¥è­˜é»æ•¸é‡: {db_stats.get('total_active', 0)}")
            logger.info(f"  æˆåŠŸé·ç§»æ•¸é‡: {self.stats['migrated_points']}")
            logger.info(f"  éŒ¯èª¤æ•¸é‡: {self.stats['errors']}")

            # æª¢æŸ¥é—œéµçµ±è¨ˆ
            success_rate = (
                (self.stats["migrated_points"] / self.stats["total_points"]) * 100
                if self.stats["total_points"] > 0
                else 0
            )
            logger.info(f"  é·ç§»æˆåŠŸç‡: {success_rate:.1f}%")

            # é©—è­‰è³‡æ–™å®Œæ•´æ€§ (æª¢æŸ¥å‰å¹¾å€‹çŸ¥è­˜é»)
            logger.info("é©—è­‰è³‡æ–™å®Œæ•´æ€§...")
            sample_size = min(5, len(json_points))
            for i in range(sample_size):
                json_point = json_points[i]
                key_point = json_point.get("key_point", "")

                # åœ¨è³‡æ–™åº«ä¸­æœå°‹
                db_points = await self.repository.search(key_point)
                matching_points = [dp for dp in db_points if dp.key_point == key_point]

                if matching_points:
                    db_point = matching_points[0]
                    logger.debug(f"âœ“ çŸ¥è­˜é»é©—è­‰é€šé: {key_point}")

                    # è©³ç´°æ¯”è¼ƒ (å¯é¸)
                    if json_point.get("explanation") != db_point.explanation:
                        logger.warning(f"èªªæ˜ä¸åŒ¹é…: {key_point}")
                else:
                    logger.warning(f"âœ— çŸ¥è­˜é»æœªæ‰¾åˆ°: {key_point}")

            return success_rate > 90  # æˆåŠŸç‡è¶…é90%è¦–ç‚ºé€šé

        except Exception as e:
            logger.error(f"é©—è­‰éç¨‹å¤±æ•—: {e}")
            return False

    def print_migration_report(self):
        """å°å‡ºé·ç§»å ±å‘Š"""
        duration = None
        if self.stats["start_time"] and self.stats["end_time"]:
            duration = self.stats["end_time"] - self.stats["start_time"]

        print("\n" + "=" * 60)
        print("è³‡æ–™é·ç§»å®Œæˆå ±å‘Š")
        print("=" * 60)
        print(f"ç¸½çŸ¥è­˜é»æ•¸é‡: {self.stats['total_points']}")
        print(f"æˆåŠŸé·ç§»æ•¸é‡: {self.stats['migrated_points']}")
        print(f"éŒ¯èª¤æ•¸é‡: {self.stats['errors']}")

        if self.stats["total_points"] > 0:
            success_rate = (self.stats["migrated_points"] / self.stats["total_points"]) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")

        if duration:
            print(f"é·ç§»æ™‚é–“: {duration.total_seconds():.1f} ç§’")

        if self.stats["errors"] == 0:
            print("âœ… é·ç§»å®Œæˆï¼Œç„¡éŒ¯èª¤")
        else:
            print(f"âš ï¸  é·ç§»å®Œæˆï¼Œæœ‰ {self.stats['errors']} å€‹éŒ¯èª¤")

        print("=" * 60)


async def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description="JSON åˆ° PostgreSQL è³‡æ–™é·ç§»å·¥å…·")
    parser.add_argument("--json-file", help="æŒ‡å®š JSON æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--batch-size", type=int, default=10, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--dry-run", action="store_true", help="åƒ…é©—è­‰ï¼Œä¸åŸ·è¡Œé·ç§»")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é·ç§»ï¼Œå¿½ç•¥ç¾æœ‰è³‡æ–™")
    parser.add_argument("--verify-only", action="store_true", help="åƒ…åŸ·è¡Œé©—è­‰")

    args = parser.parse_args()

    # ç¢ºå®š JSON æª”æ¡ˆè·¯å¾‘
    if args.json_file:
        json_file = Path(args.json_file)
    else:
        json_file = DATA_DIR / "knowledge.json"

    migrator = DataMigrator()

    try:
        # åˆå§‹åŒ–
        logger.info("æ­£åœ¨åˆå§‹åŒ–è³‡æ–™åº«é€£ç·š...")
        if not await migrator.initialize():
            logger.error("åˆå§‹åŒ–å¤±æ•—")
            sys.exit(1)

        # è¼‰å…¥ JSON è³‡æ–™
        logger.info(f"æ­£åœ¨è¼‰å…¥ JSON è³‡æ–™: {json_file}")
        json_data = migrator.load_json_data(json_file)
        if not json_data:
            logger.error("è¼‰å…¥ JSON è³‡æ–™å¤±æ•—")
            sys.exit(1)

        if args.verify_only:
            # åƒ…é©—è­‰
            logger.info("åŸ·è¡Œé©—è­‰æ¨¡å¼...")
            success = await migrator.verify_migration(json_data)
            if success:
                logger.info("âœ… é©—è­‰é€šé")
                sys.exit(0)
            else:
                logger.error("âŒ é©—è­‰å¤±æ•—")
                sys.exit(1)

        if args.dry_run:
            # åƒ…æª¢æŸ¥ï¼Œä¸åŸ·è¡Œ
            logger.info("åŸ·è¡Œä¹¾è·‘æ¨¡å¼ (ä¸æœƒå¯¦éš›é·ç§»è³‡æ–™)...")
            points_count = len(json_data.get("data", []))
            logger.info(f"å°‡è¦é·ç§» {points_count} å€‹çŸ¥è­˜é»")
            logger.info("ä¹¾è·‘å®Œæˆï¼ŒæœªåŸ·è¡Œå¯¦éš›é·ç§»")
            sys.exit(0)

        # ç¢ºèªé·ç§»
        if not args.force:
            points_count = len(json_data.get("data", []))
            response = input(f"å³å°‡é·ç§» {points_count} å€‹çŸ¥è­˜é»åˆ°è³‡æ–™åº«ï¼Œæ˜¯å¦ç¹¼çºŒï¼Ÿ(y/N): ")
            if response.lower() != "y":
                logger.info("å–æ¶ˆé·ç§»")
                sys.exit(0)

        # åŸ·è¡Œé·ç§»
        logger.info("é–‹å§‹åŸ·è¡Œè³‡æ–™é·ç§»...")
        success = await migrator.migrate_knowledge_points(json_data, args.batch_size)

        if success:
            # é©—è­‰é·ç§»çµæœ
            verification_success = await migrator.verify_migration(json_data)

            # å°å‡ºå ±å‘Š
            migrator.print_migration_report()

            if verification_success:
                logger.info("ğŸ‰ é·ç§»æˆåŠŸå®Œæˆä¸¦é€šéé©—è­‰")
                sys.exit(0)
            else:
                logger.warning("âš ï¸  é·ç§»å®Œæˆä½†é©—è­‰æœªé€šéï¼Œè«‹æª¢æŸ¥")
                sys.exit(1)
        else:
            logger.error("âŒ é·ç§»å¤±æ•—")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.warning("é·ç§»è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        logger.error(f"é·ç§»éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)
    finally:
        await migrator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
