"""
å¯¦éš›æ¶æ§‹ä¸€è‡´æ€§åˆ†æè…³æœ¬
åŸºæ–¼çœŸå¯¦æ•¸æ“šå’Œæ–¹æ³•èª¿ç”¨é€²è¡Œæ¶æ§‹å°æ¯”åˆ†æ
"""

import asyncio
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from core.database.adapter import KnowledgeManagerAdapter
    from core.error_types import ErrorCategory
    from core.knowledge import KnowledgeManager
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")
    sys.exit(1)


class RealArchitectureAnalyzer:
    """çœŸå¯¦æ¶æ§‹ä¸€è‡´æ€§åˆ†æå™¨"""

    def __init__(self):
        self.json_manager = KnowledgeManager()
        self.db_manager = KnowledgeManagerAdapter(use_database=True)
        self.analysis_results = {}

    async def run_complete_analysis(self) -> dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ¶æ§‹ä¸€è‡´æ€§åˆ†æ"""
        print("ğŸ” é–‹å§‹çœŸå¯¦æ¶æ§‹ä¸€è‡´æ€§åˆ†æ...")
        print("=" * 60)

        start_time = time.time()

        # åˆ†æå„å€‹å±¤é¢
        results = {
            "timestamp": datetime.now().isoformat(),
            "api_interface_analysis": await self._analyze_api_interfaces(),
            "data_model_analysis": self._analyze_data_models(),
            "statistics_consistency": await self._analyze_statistics_consistency(),
            "performance_comparison": await self._analyze_performance(),
            "method_availability": self._analyze_method_availability(),
            "error_handling_comparison": await self._analyze_error_handling(),
            "overall_assessment": {},
        }

        # ç”Ÿæˆæ•´é«”è©•ä¼°
        results["overall_assessment"] = self._generate_overall_assessment(results)
        results["analysis_duration"] = time.time() - start_time

        return results

    async def _analyze_api_interfaces(self) -> dict[str, Any]:
        """åˆ†æAPIæ¥å£ä¸€è‡´æ€§"""
        print("\nğŸ“‹ 1. APIæ¥å£ä¸€è‡´æ€§åˆ†æ")

        # æª¢æŸ¥æ ¸å¿ƒæ–¹æ³•å­˜åœ¨æ€§
        core_methods = [
            "get_statistics",
            "get_active_points",
            "get_review_candidates",
            "get_recommendations",
            "edit_knowledge_point",
            "delete_point",
            "restore_point",
        ]

        method_comparison = {}
        for method in core_methods:
            json_has = hasattr(self.json_manager, method)

            # æª¢æŸ¥Databaseç®¡ç†å™¨çš„ç•°æ­¥ç‰ˆæœ¬
            async_method = (
                method + "_async" if method != "get_active_points" else "get_knowledge_points_async"
            )
            db_has = hasattr(self.db_manager, async_method)

            method_comparison[method] = {
                "json_has": json_has,
                "db_has": db_has,
                "both_available": json_has and db_has,
            }

        available_methods = sum(1 for m in method_comparison.values() if m["both_available"])
        compatibility_score = available_methods / len(core_methods)

        print(
            f"   æ ¸å¿ƒæ–¹æ³•å…¼å®¹æ€§: {compatibility_score:.1%} ({available_methods}/{len(core_methods)})"
        )

        return {
            "method_comparison": method_comparison,
            "compatibility_score": compatibility_score,
            "total_methods_checked": len(core_methods),
        }

    def _analyze_data_models(self) -> dict[str, Any]:
        """åˆ†ææ•¸æ“šæ¨¡å‹ä¸€è‡´æ€§"""
        print("\nğŸ“Š 2. æ•¸æ“šæ¨¡å‹ä¸€è‡´æ€§åˆ†æ")

        # æª¢æŸ¥éŒ¯èª¤åˆ†é¡ç³»çµ±
        try:
            categories = [cat.value for cat in ErrorCategory]
            print(f"   éŒ¯èª¤åˆ†é¡ç³»çµ±: {len(categories)} ç¨®åˆ†é¡")

            return {
                "error_categories": categories,
                "category_count": len(categories),
                "model_consistency": True,  # å…©ç¨®æ¨¡å¼ä½¿ç”¨ç›¸åŒçš„æ•¸æ“šæ¨¡å‹
                "note": "å…©ç¨®æ¨¡å¼ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ KnowledgePoint å’Œ ErrorCategory æ¨¡å‹",
            }
        except Exception as e:
            return {"error": str(e), "model_consistency": False}

    async def _analyze_statistics_consistency(self) -> dict[str, Any]:
        """åˆ†æçµ±è¨ˆæ•¸æ“šä¸€è‡´æ€§"""
        print("\nğŸ“ˆ 3. çµ±è¨ˆæ•¸æ“šä¸€è‡´æ€§åˆ†æ")

        try:
            # ç²å–å…©ç¨®æ¨¡å¼çš„çµ±è¨ˆæ•¸æ“š
            json_stats = self.json_manager.get_statistics()
            db_stats = await self.db_manager.get_statistics_async()

            # æ¯”è¼ƒé—œéµæŒ‡æ¨™
            key_metrics = ["knowledge_points", "total_practices", "correct_count", "mistake_count"]
            metric_comparison = {}

            for metric in key_metrics:
                json_val = json_stats.get(metric, 0)
                db_val = db_stats.get(metric, 0)

                metric_comparison[metric] = {
                    "json": json_val,
                    "db": db_val,
                    "match": json_val == db_val,
                    "diff_percent": abs(json_val - db_val) / max(json_val, db_val, 1) * 100,
                }

            # è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸
            matching_metrics = sum(1 for m in metric_comparison.values() if m["match"])
            consistency_score = matching_metrics / len(key_metrics)

            print(
                f"   çµ±è¨ˆä¸€è‡´æ€§: {consistency_score:.1%} ({matching_metrics}/{len(key_metrics)} æŒ‡æ¨™åŒ¹é…)"
            )

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§
            json_keys = set(json_stats.keys())
            db_keys = set(db_stats.keys())
            format_consistency = len(json_keys & db_keys) / len(json_keys | db_keys)

            return {
                "metric_comparison": metric_comparison,
                "consistency_score": consistency_score,
                "format_consistency": format_consistency,
                "json_stats": json_stats,
                "db_stats": db_stats,
            }

        except Exception as e:
            print(f"   âŒ çµ±è¨ˆåˆ†æå¤±æ•—: {e}")
            return {"error": str(e), "consistency_score": 0.0}

    async def _analyze_performance(self) -> dict[str, Any]:
        """åˆ†ææ€§èƒ½ç‰¹å¾µ"""
        print("\nâš¡ 4. æ€§èƒ½ç‰¹å¾µæ¯”è¼ƒ")

        performance_results = {}

        try:
            # æ¸¬è©¦çµ±è¨ˆè¨ˆç®—æ€§èƒ½
            start = time.time()
            self.json_manager.get_statistics()
            json_stats_time = time.time() - start

            start = time.time()
            await self.db_manager.get_statistics_async()
            db_stats_time = time.time() - start

            performance_results["statistics_calculation"] = {
                "json_time": json_stats_time,
                "db_time": db_stats_time,
                "ratio": db_stats_time / max(json_stats_time, 0.001),
            }

            # æ¸¬è©¦çŸ¥è­˜é»æª¢ç´¢æ€§èƒ½
            start = time.time()
            self.json_manager.get_active_points()
            json_retrieval_time = time.time() - start

            start = time.time()
            await self.db_manager.get_knowledge_points_async()
            db_retrieval_time = time.time() - start

            performance_results["data_retrieval"] = {
                "json_time": json_retrieval_time,
                "db_time": db_retrieval_time,
                "ratio": db_retrieval_time / max(json_retrieval_time, 0.001),
            }

            # è¨ˆç®—å¹³å‡æ€§èƒ½æ¯”ç‡
            avg_ratio = sum(metric["ratio"] for metric in performance_results.values()) / len(
                performance_results
            )

            print(
                f"   çµ±è¨ˆè¨ˆç®—: JSON={json_stats_time:.4f}s, DB={db_stats_time:.4f}s (æ¯”ç‡: {performance_results['statistics_calculation']['ratio']:.2f})"
            )
            print(
                f"   æ•¸æ“šæª¢ç´¢: JSON={json_retrieval_time:.4f}s, DB={db_retrieval_time:.4f}s (æ¯”ç‡: {performance_results['data_retrieval']['ratio']:.2f})"
            )
            print(f"   å¹³å‡æ€§èƒ½æ¯”ç‡: {avg_ratio:.2f}")

            performance_results["average_ratio"] = avg_ratio
            performance_results["performance_score"] = (
                min(1.0, 2.0 / avg_ratio) if avg_ratio > 0 else 1.0
            )

        except Exception as e:
            print(f"   âŒ æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            performance_results = {"error": str(e), "performance_score": 0.0}

        return performance_results

    def _analyze_method_availability(self) -> dict[str, Any]:
        """åˆ†ææ–¹æ³•å¯ç”¨æ€§"""
        print("\nğŸ”§ 5. æ–¹æ³•å¯ç”¨æ€§åˆ†æ")

        # ç²å–æ‰€æœ‰æ–¹æ³•
        json_methods = [
            method
            for method in dir(self.json_manager)
            if not method.startswith("_") and callable(getattr(self.json_manager, method))
        ]
        db_methods = [
            method
            for method in dir(self.db_manager)
            if not method.startswith("_") and callable(getattr(self.db_manager, method))
        ]

        common_methods = set(json_methods) & set(db_methods)
        json_only = set(json_methods) - set(db_methods)
        db_only = set(db_methods) - set(json_methods)

        print(f"   å…±åŒæ–¹æ³•: {len(common_methods)} å€‹")
        print(f"   JSONç¨æœ‰: {len(json_only)} å€‹")
        print(f"   DBç¨æœ‰: {len(db_only)} å€‹")

        return {
            "json_methods": len(json_methods),
            "db_methods": len(db_methods),
            "common_methods": len(common_methods),
            "json_only_methods": list(json_only),
            "db_only_methods": list(db_only),
            "method_overlap": len(common_methods) / max(len(json_methods), len(db_methods)),
        }

    async def _analyze_error_handling(self) -> dict[str, Any]:
        """åˆ†æéŒ¯èª¤è™•ç†ä¸€è‡´æ€§"""
        print("\nâš ï¸  6. éŒ¯èª¤è™•ç†ä¸€è‡´æ€§åˆ†æ")

        # æ¸¬è©¦ç„¡æ•ˆIDè™•ç†
        try:
            json_result = self.json_manager.edit_knowledge_point(99999, {"key_point": "test"})
            json_handles_invalid_id = json_result is None or json_result is False
        except Exception:
            json_handles_invalid_id = True

        try:
            db_result = await self.db_manager.get_knowledge_point_async("99999")
            db_handles_invalid_id = db_result is None or db_result is False
        except Exception:
            db_handles_invalid_id = True

        invalid_id_consistent = json_handles_invalid_id == db_handles_invalid_id

        print(f"   ç„¡æ•ˆIDè™•ç†: {'ä¸€è‡´' if invalid_id_consistent else 'ä¸ä¸€è‡´'}")

        return {
            "invalid_id_handling": {
                "json_handles": json_handles_invalid_id,
                "db_handles": db_handles_invalid_id,
                "consistent": invalid_id_consistent,
            },
            "overall_consistency": 1.0 if invalid_id_consistent else 0.0,
        }

    def _generate_overall_assessment(self, results: dict[str, Any]) -> dict[str, Any]:
        """ç”Ÿæˆæ•´é«”è©•ä¼°"""
        print("\nğŸ“‹ 7. æ•´é«”æ¶æ§‹ä¸€è‡´æ€§è©•ä¼°")

        # æå–å„é …åˆ†æ•¸
        scores = {
            "api_compatibility": results.get("api_interface_analysis", {}).get(
                "compatibility_score", 0.0
            ),
            "data_model_consistency": 1.0
            if results.get("data_model_analysis", {}).get("model_consistency")
            else 0.0,
            "statistics_consistency": results.get("statistics_consistency", {}).get(
                "consistency_score", 0.0
            ),
            "performance_consistency": results.get("performance_comparison", {}).get(
                "performance_score", 0.0
            ),
            "method_availability": results.get("method_availability", {}).get(
                "method_overlap", 0.0
            ),
            "error_handling": results.get("error_handling_comparison", {}).get(
                "overall_consistency", 0.0
            ),
        }

        # æ¬Šé‡è¨ˆç®—æ•´é«”åˆ†æ•¸
        weights = {
            "api_compatibility": 0.25,
            "data_model_consistency": 0.20,
            "statistics_consistency": 0.25,
            "performance_consistency": 0.10,
            "method_availability": 0.15,
            "error_handling": 0.05,
        }

        overall_score = sum(scores[key] * weights[key] for key in scores)

        # è©•ç´š
        if overall_score >= 0.95:
            grade = "ğŸ† å¹¾ä¹å®Œå…¨ç›¸åŒ"
        elif overall_score >= 0.85:
            grade = "ğŸ¥ˆ é«˜åº¦ç›¸ä¼¼"
        elif overall_score >= 0.70:
            grade = "ğŸ¥‰ åŸºæœ¬ä¸€è‡´"
        else:
            grade = "âš ï¸ å­˜åœ¨é¡¯è‘—å·®ç•°"

        assessment = {
            "individual_scores": scores,
            "overall_score": overall_score,
            "grade": grade,
            "weights": weights,
        }

        # æ‰“å°è©•ä¼°çµæœ
        print(f"   APIå…¼å®¹æ€§: {scores['api_compatibility']:.1%}")
        print(f"   æ•¸æ“šæ¨¡å‹: {scores['data_model_consistency']:.1%}")
        print(f"   çµ±è¨ˆä¸€è‡´æ€§: {scores['statistics_consistency']:.1%}")
        print(f"   æ€§èƒ½ä¸€è‡´æ€§: {scores['performance_consistency']:.1%}")
        print(f"   æ–¹æ³•å¯ç”¨æ€§: {scores['method_availability']:.1%}")
        print(f"   éŒ¯èª¤è™•ç†: {scores['error_handling']:.1%}")
        print(f"   æ•´é«”è©•åˆ†: {overall_score:.1%}")
        print(f"   è©•ç´š: {grade}")

        return assessment


async def main():
    """ä¸»å‡½æ•¸"""
    analyzer = RealArchitectureAnalyzer()

    print("ğŸ—ï¸  JSON vs Database æ¶æ§‹ä¸€è‡´æ€§çœŸå¯¦åˆ†æ")
    print("=" * 60)

    try:
        results = await analyzer.run_complete_analysis()

        # ä¿å­˜çµæœ
        output_file = "real_architecture_analysis.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\n{'=' * 60}")
        print("ğŸ“‹ æœ€çµ‚çµè«–")
        print("=" * 60)

        overall = results["overall_assessment"]
        print(f"ğŸ¯ æ•´é«”ä¸€è‡´æ€§è©•åˆ†: {overall['overall_score']:.1%}")
        print(f"ğŸ“Š è©•ç´š: {overall['grade']}")
        print(f"â±ï¸  åˆ†æè€—æ™‚: {results['analysis_duration']:.2f}ç§’")

        # å›ç­”æ ¸å¿ƒå•é¡Œ
        if overall["overall_score"] >= 0.95:
            answer = "âœ… æ˜¯çš„ï¼Œå…©å€‹æ¶æ§‹å¹¾ä¹å®Œå…¨ç›¸åŒ"
        elif overall["overall_score"] >= 0.85:
            answer = "âœ… æ¶æ§‹é«˜åº¦ç›¸ä¼¼ï¼Œå­˜åœ¨å°å¹…å·®ç•°ä½†åŸºæœ¬ä¸€è‡´"
        elif overall["overall_score"] >= 0.70:
            answer = "âš ï¸ æ¶æ§‹åŸºæœ¬ä¸€è‡´ï¼Œéœ€è¦æ³¨æ„éƒ¨åˆ†åŠŸèƒ½å·®ç•°"
        else:
            answer = "âŒ æ¶æ§‹å­˜åœ¨é¡¯è‘—å·®ç•°ï¼Œéœ€è¦é‡é»é—œæ³¨å…¼å®¹æ€§"

        print(f"\nğŸ¯ å•é¡Œç­”æ¡ˆ: {answer}")
        print(f"ğŸ“„ è©³ç´°çµæœå·²ä¿å­˜è‡³: {output_file}")

        return overall["overall_score"] >= 0.85

    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
