#!/usr/bin/env python3
"""
æ•¸æ“šåŒæ­¥è…³æœ¬ - è§£æ±º JSON å’Œ Database ID ä¸åŒ¹é…å•é¡Œ
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

import psycopg2

from core.database.connection import DatabaseSettings
from core.knowledge import KnowledgeManager
from scripts.data_check import analyze_consistency


async def sync_knowledge_ids():
    """åŒæ­¥çŸ¥è­˜é» IDï¼Œä»¥ JSON ç‚ºä¸»ï¼Œæ›´æ–° Database"""
    print("ğŸ”„ é–‹å§‹åŒæ­¥ JSON å’Œ Database çŸ¥è­˜é» ID...")

    # 1. åˆ†æç•¶å‰ç‹€æ…‹
    analysis = await analyze_consistency()
    if not analysis:
        print("âŒ ç„¡æ³•ç²å–åˆ†æçµæœï¼Œåœæ­¢åŒæ­¥")
        return False

    print(f"\nğŸ“Š ç•¶å‰ç‹€æ…‹ï¼šJSON({analysis['json_count']}) vs Database({analysis['db_count']})")

    if not analysis["only_in_json"] and not analysis["only_in_db"]:
        print("âœ… æ•¸æ“šå·²ç¶“ä¸€è‡´ï¼Œç„¡éœ€åŒæ­¥")
        return True

    # 2. è¼‰å…¥ JSON æ•¸æ“šä½œç‚ºæ¬Šå¨ä¾†æº
    json_manager = KnowledgeManager(data_dir="data")
    json_points = json_manager.knowledge_points

    # 3. é€£æ¥è³‡æ–™åº«æº–å‚™åŒæ­¥
    db_settings = DatabaseSettings()

    try:
        with psycopg2.connect(db_settings.DATABASE_URL) as conn, conn.cursor() as cur:
            print("\nğŸ—„ï¸  æ¸…ç† Database ä¸­çš„çŸ¥è­˜é»...")

            # å®Œå…¨æ¸…ç©ºç¾æœ‰çš„çŸ¥è­˜é»å’Œç›¸é—œæ•¸æ“šï¼ˆé¿å…å”¯ä¸€ç´„æŸè¡çªï¼‰
            print("   æ¸…ç†æ‰€æœ‰ç›¸é—œè¡¨...")
            cur.execute("DELETE FROM knowledge_point_versions")
            cur.execute("DELETE FROM knowledge_point_tags")
            cur.execute("DELETE FROM practice_queue")
            cur.execute("DELETE FROM review_examples")
            cur.execute("DELETE FROM original_errors")
            cur.execute("DELETE FROM knowledge_points")

            # æš«æ™‚åˆªé™¤å”¯ä¸€ç´„æŸç´¢å¼•
            cur.execute("DROP INDEX IF EXISTS uk_kp_content")
            print("   å·²å®Œå…¨æ¸…ç©ºæ‰€æœ‰ç›¸é—œæ•¸æ“š")
            deleted_count = cur.rowcount
            print(f"   å·²è»Ÿåˆªé™¤ {deleted_count} å€‹ç¾æœ‰çŸ¥è­˜é»")

            print("\nğŸ“ æ’å…¥ JSON çŸ¥è­˜é»åˆ° Database...")
            inserted_count = 0

            for point in json_points:
                # å°‡ ErrorCategory enum è½‰æ›ç‚ºå­—ç¬¦ä¸²
                category_str = str(point.category).split(".")[-1].lower()

                # ä¿®æ­£æ™‚é–“ç´„æŸå•é¡Œï¼šç¢ºä¿ next_review >= last_seen
                from datetime import datetime

                try:
                    if point.next_review and point.last_seen:
                        next_review_dt = datetime.fromisoformat(
                            point.next_review.replace("Z", "+00:00")
                        )
                        last_seen_dt = datetime.fromisoformat(
                            point.last_seen.replace("Z", "+00:00")
                        )
                        # ç§»é™¤æ™‚å€ä¿¡æ¯é€²è¡Œæ¯”è¼ƒ
                        next_review_dt = next_review_dt.replace(tzinfo=None)
                        last_seen_dt = last_seen_dt.replace(tzinfo=None)

                        if next_review_dt < last_seen_dt:
                            # ä¿®æ­£ï¼šå°‡ next_review è¨­ç‚º last_seen çš„æ™‚é–“
                            point.next_review = point.last_seen
                            print(f"   ä¿®æ­£çŸ¥è­˜é» {point.id} çš„æ™‚é–“ç´„æŸ")
                except (ValueError, AttributeError):
                    # æ™‚é–“æ ¼å¼æœ‰å•é¡Œï¼Œè¨­ç‚ºç•¶å‰æ™‚é–“
                    current_time = datetime.now().isoformat()
                    point.next_review = current_time
                    point.last_seen = current_time

                # æ’å…¥çŸ¥è­˜é» (å°æ‡‰è³‡æ–™åº« schema)
                cur.execute(
                    """
                        INSERT INTO knowledge_points (
                            id, key_point, category, subtype, explanation,
                            original_phrase, correction, mastery_level,
                            next_review, mistake_count, correct_count, custom_notes,
                            is_deleted, created_at, last_modified, last_seen
                        ) VALUES (
                            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                            FALSE, %s, CURRENT_TIMESTAMP, %s
                        ) ON CONFLICT (id) DO UPDATE SET
                            key_point = EXCLUDED.key_point,
                            category = EXCLUDED.category,
                            subtype = EXCLUDED.subtype,
                            explanation = EXCLUDED.explanation,
                            original_phrase = EXCLUDED.original_phrase,
                            correction = EXCLUDED.correction,
                            mastery_level = EXCLUDED.mastery_level,
                            next_review = EXCLUDED.next_review,
                            last_seen = EXCLUDED.last_seen,
                            mistake_count = EXCLUDED.mistake_count,
                            correct_count = EXCLUDED.correct_count,
                            custom_notes = EXCLUDED.custom_notes || ' [å¾JSONåŒæ­¥]',
                            is_deleted = FALSE,
                            last_modified = CURRENT_TIMESTAMP
                    """,
                    (
                        point.id,
                        point.key_point,
                        category_str,
                        point.subtype,
                        point.explanation,
                        point.original_phrase,
                        point.correction,
                        point.mastery_level,
                        point.next_review,
                        point.mistake_count,
                        point.correct_count,
                        point.custom_notes,
                        point.created_at,
                        point.last_seen,
                    ),
                )

                inserted_count += 1

            print(f"   å·²æ’å…¥/æ›´æ–° {inserted_count} å€‹çŸ¥è­˜é»")

            # æ›´æ–° ID åºåˆ—ï¼ˆç¢ºä¿ä¸‹æ¬¡æ’å…¥ä½¿ç”¨æ­£ç¢ºçš„ IDï¼‰
            cur.execute("""
                    SELECT setval('knowledge_points_id_seq',
                                 COALESCE((SELECT MAX(id) FROM knowledge_points), 1))
                """)

            # æäº¤äº‹å‹™
            conn.commit()
            print("   âœ… æ•¸æ“šåŒæ­¥å®Œæˆ")

    except Exception as e:
        print(f"âŒ æ•¸æ“šåŒæ­¥å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False

    # 4. é©—è­‰åŒæ­¥çµæœ
    print("\nğŸ” é©—è­‰åŒæ­¥çµæœ...")
    final_analysis = await analyze_consistency()

    if final_analysis and final_analysis["json_count"] == final_analysis["db_count"]:
        print(f"âœ… åŒæ­¥æˆåŠŸï¼å…©ç¨®æ¨¡å¼éƒ½åŒ…å« {final_analysis['json_count']} å€‹çŸ¥è­˜é»")
        return True
    else:
        print("âŒ åŒæ­¥å¾Œä»æœ‰å·®ç•°ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æŸ¥")
        return False


async def backup_database_before_sync():
    """åœ¨åŒæ­¥å‰å‚™ä»½è³‡æ–™åº«æ•¸æ“š"""
    print("ğŸ’¾ åœ¨åŒæ­¥å‰å‚™ä»½ Database æ•¸æ“š...")

    db_settings = DatabaseSettings()
    backup_data = []

    try:
        with psycopg2.connect(db_settings.DATABASE_URL) as conn, conn.cursor() as cur:
            cur.execute("""
                    SELECT id, key_point, category, created_at, is_deleted
                    FROM knowledge_points
                    ORDER BY id
                """)

            rows = cur.fetchall()
            for row in rows:
                backup_data.append(
                    {
                        "id": row[0],
                        "key_point": row[1],
                        "category": row[2],
                        "created_at": str(row[3]),
                        "is_deleted": row[4],
                    }
                )

        # å¯«å…¥å‚™ä»½æ–‡ä»¶
        import json
        from datetime import datetime

        backup_file = (
            f"data/backups/knowledge_pre_sync_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        Path(backup_file).parent.mkdir(exist_ok=True)

        with open(backup_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "backup_time": datetime.now().isoformat(),
                    "total_count": len(backup_data),
                    "data": backup_data,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"   âœ… å·²å‚™ä»½ {len(backup_data)} å€‹çŸ¥è­˜é»åˆ° {backup_file}")
        return True

    except Exception as e:
        print(f"âŒ å‚™ä»½å¤±æ•—: {e}")
        return False


if __name__ == "__main__":

    async def main():
        # 1. å‚™ä»½ç¾æœ‰æ•¸æ“š
        if not await backup_database_before_sync():
            print("âŒ å‚™ä»½å¤±æ•—ï¼Œåœæ­¢åŒæ­¥æ“ä½œ")
            return

        # 2. åŸ·è¡ŒåŒæ­¥
        success = await sync_knowledge_ids()

        if success:
            print("\nğŸ‰ TASK-19C å®Œæˆï¼JSON å’Œ Database çŸ¥è­˜é»æ•¸é‡å·²åŒæ­¥")
        else:
            print("\nâŒ åŒæ­¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¸¦æ‰‹å‹•è™•ç†")

    asyncio.run(main())
